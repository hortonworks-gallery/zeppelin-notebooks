{
  "paragraphs":[
    {
      "text":"%md\n\n## Intro to Spark SQL and DataFrames with Scala\n#### Exploring an Airline Dataset\nby Robert Hryniewicz\nver 0.2\n",
      "dateUpdated":"Apr 10, 2016 10:48:24 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298133_-1858850818",
      "id":"20160410-003138_1880368561",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<h2>Intro to Spark SQL and DataFrames with Scala</h2>\n<h4>Exploring an Airline Dataset</h4>\n<p>by Robert Hryniewicz\n<br  />ver 0.2</p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "dateStarted":"Apr 10, 2016 10:48:23 AM",
      "dateFinished":"Apr 10, 2016 10:48:23 AM",
      "status":"FINISHED",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:54"
    },
    {
      "text":"%md\n### Introduction\n\nIn this lab you will use Spark SQL via DataFrames API in Part 1 of the lab and SQL in Part 2 of the lab to explore an Airline Dataset. This is a very interesting dataset that is further explored in a more advanced lab by applying Machine Learning methods for predictive analytics.",
      "dateUpdated":"Apr 10, 2016 10:48:17 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":217,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298133_-1858850818",
      "id":"20160410-003138_985055475",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<h3>Introduction</h3>\n<p>In this lab you will use Spark SQL via DataFrames API in Part 1 of the lab and SQL in Part 2 of the lab to explore an Airline Dataset. This is a very interesting dataset that is further explored in a more advanced lab by applying Machine Learning methods for predictive analytics.</p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "dateStarted":"Apr 10, 2016 10:48:12 AM",
      "dateFinished":"Apr 10, 2016 10:48:12 AM",
      "status":"FINISHED",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:55"
    },
    {
      "text":"%md\n### Concepts\n\nA DataFrame is a distributed collection of data organized into named columns. \n#\nIt is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. \n#\nDataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. \n#\n**[See SparkSQL docs for more info](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes)**",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298133_-1858850818",
      "id":"20160410-003138_875933602",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<h3>Concepts</h3>\n<p>A DataFrame is a distributed collection of data organized into named columns.</p>\n<h1></h1>\n<p>It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.</p>\n<h1></h1>\n<p>DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.</p>\n<h1></h1>\n<p><strong><a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\">See SparkSQL docs for more info</a></strong></p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:56"
    },
    {
      "config":{
        "colWidth":12,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "enabled":true,
        "editorMode":"ace/mode/markdown",
        "editorHide":true
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460297036018_-1630181514",
      "id":"20160410-140356_736870357",
      "dateCreated":"Apr 10, 2016 2:03:56 PM",
      "status":"FINISHED",
      "progressUpdateIntervalMs":500,
      "focus":true,
      "$$hashKey":"object:1110",
      "text":"%md\n\nThroughout this lab we will use basic Scala syntax. If you would like to learn more about Scala, here's an excellent [Scala Basics Tutorial](http://www.dhgarrette.com/nlpclass/scala/basics.html).",
      "dateUpdated":"Apr 10, 2016 2:07:20 PM",
      "dateFinished":"Apr 10, 2016 2:07:19 PM",
      "dateStarted":"Apr 10, 2016 2:07:19 PM",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<p>Throughout this lab we will use basic Scala syntax. If you would like to learn more about Scala, here's an excellent <a href=\"http://www.dhgarrette.com/nlpclass/scala/basics.html\">Scala Basics Tutorial</a>.</p>\n"
      }
    },
    {
      "text":"%md\n### Lab Setup & Pre-Check\nBefore we proceed, let's set Spark's external package dependencies and then verify the Spark Version (you should be running at minimum Spark 1.6 for this lab).",
      "dateUpdated":"Apr 10, 2016 10:50:35 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298133_-1858850818",
      "id":"20160410-003138_815648629",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<h3>Lab Setup &amp; Pre-Check</h3>\n<p>Before we proceed, let's set Spark's external package dependencies and then verify the Spark Version (you should be running at minimum Spark 1.6 for this lab).</p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "dateStarted":"Apr 10, 2016 10:50:35 AM",
      "dateFinished":"Apr 10, 2016 10:50:35 AM",
      "status":"FINISHED",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:57"
    },
    {
      "text":"%md\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298133_-1858850818",
      "id":"20160410-003138_1218388802",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:58"
    },
    {
      "title":"Set External Spark Package Dependencies",
      "text":"%dep\n\n// IMPORTANT! \n// This step/paragraph must be executed FIRST; if you have already executed other commands/paragraphs, \n//   please click \"Interpreter\" in the menu above and restart the \"spark\" interpreter and then run this paragraph\n//   before any other one.\n\nz.reset()\nz.load(\"com.databricks:spark-csv_2.11:1.4.0\")   // Spark CSV package",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "editorHide":false,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298134_-1857696571",
      "id":"20160410-003138_1204021069",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:59"
    },
    {
      "text":"%md\n**Note**: The first time you run `sc.version` in the paragraph below, several services will initialize in the background. \nThis may take **1~2 min** so please **be patient**. Afterwards, each paragraph should run much more quickly since all the services will already be running.",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298134_-1857696571",
      "id":"20160410-003138_243701002",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<p><strong>Note</strong>: The first time you run <code>sc.version</code> in the paragraph below, several services will initialize in the background.\n<br  />This may take <strong>1~2 min</strong> so please <strong>be patient</strong>. Afterwards, each paragraph should run much more quickly since all the services will already be running.</p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:60"
    },
    {
      "title":"Check Spark Version",
      "text":"sc.version",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298134_-1857696571",
      "id":"20160410-003138_631425785",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:61"
    },
    {
      "text":"%md ###Start of Lab\n",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298134_-1857696571",
      "id":"20160410-003138_1656468668",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<h3>Start of Lab</h3>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:62"
    },
    {
      "text":"%md\nIn the next paragraph we are going to download datasets using shell commands. A shell command in a Zeppelin notebook can can be invoked by \nprepending a block of shell commands with a line containing `%sh` characters.",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298134_-1857696571",
      "id":"20160410-003138_290903368",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<p>In the next paragraph we are going to download datasets using shell commands. A shell command in a Zeppelin notebook can can be invoked by\n<br  />prepending a block of shell commands with a line containing <code>%sh</code> characters.</p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:63"
    },
    {
      "title":"Download Datasets",
      "text":"%sh\n\n# You will now download a subset of 2008 flights (only 100k lines)\n# The full dataset may be found here: http://stat-computing.org/dataexpo/2009/the-data.html\n\nwget https://raw.githubusercontent.com/roberthryniewicz/datasets/master/airline-dataset/flights/flights.csv -O /tmp/flights.csv\necho \"Downloaded!\"",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "tableHide":false,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/sh",
        "editorHide":false,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298134_-1857696571",
      "id":"20160410-003138_1540125404",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:64"
    },
    {
      "title":"Move Datasets to HDFS",
      "text":"%sh\n\n# remove existing copies of dataset from HDFS\nhadoop fs -rm -r -f /tmp/airflightsdelays\n\n# create directory on HDFS\nhadoop fs -mkdir /tmp/airflightsdelays\n\n# put data into HDFS\nhadoop fs -put /tmp/flights.csv /tmp/airflightsdelays/",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298134_-1857696571",
      "id":"20160410-003138_1267267737",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:65"
    },
    {
      "title":"Preview Downloaded File",
      "text":"%sh\nhadoop fs -cat /tmp/airflightsdelays/flights.csv | head",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298134_-1857696571",
      "id":"20160410-003138_226044813",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:66"
    },
    {
      "text":"// Spark Context and Spark SQL Context are automatically initialized in Zeppelin so we will skip those steps\n//val sc: Spark Context\n//val sqlContext: SQL Context\n\n// Create a DataFrame from datasets\nval df = sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\")       // Use first line of all files as header\n    .option(\"inferSchema\", \"true\")  // Automatically infer data types\n    .load(\"/tmp/airflightsdelays/\") // Read all flights",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298134_-1857696571",
      "id":"20160410-003138_236600548",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:67"
    },
    {
      "text":"// Print the schema in a tree format\ndf.printSchema",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298135_-1858081320",
      "id":"20160410-003138_1553179639",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:68"
    },
    {
      "title":"Dataset Description",
      "text":"%angular\n\n<table width=\"100%\">\n<tbody><tr>\n  <th></th>\n  <th>Name</th>\n  <th>Description</th>\n</tr>\n<tr>\n <td>1  </td><td> Year              </td><td>1987-2008</td>\n</tr><tr>\n <td>2  </td><td> Month             </td><td>1-12</td>\n</tr><tr>\n <td>3  </td><td> DayofMonth        </td><td>1-31</td>\n</tr><tr>\n <td>4  </td><td> DayOfWeek         </td><td>1 (Monday) - 7 (Sunday)</td>\n</tr><tr>\n <td>5  </td><td> DepTime           </td><td>actual departure time (local, hhmm)</td>\n</tr><tr>\n <td>6  </td><td> CRSDepTime        </td><td>scheduled departure time (local, hhmm)</td>\n</tr><tr>\n <td>7  </td><td> ArrTime           </td><td>actual arrival time (local, hhmm)</td>\n</tr><tr>\n <td>8  </td><td> CRSArrTime        </td><td>scheduled arrival time (local, hhmm)</td>\n</tr><tr>\n <td>9  </td><td> UniqueCarrier     </td><td><a href=\"supplemental-data.html\">unique carrier code</a></td>\n</tr><tr>\n <td>10 </td><td> FlightNum         </td><td>flight number</td>\n</tr><tr>\n <td>11 </td><td> TailNum           </td><td>plane tail number</td>\n</tr><tr>\n <td>12 </td><td> ActualElapsedTime </td><td>in minutes</td>\n</tr><tr>\n <td>13 </td><td> CRSElapsedTime    </td><td>in minutes</td>\n</tr><tr>\n <td>14 </td><td> AirTime           </td><td>in minutes</td>\n</tr><tr>\n <td>15 </td><td> ArrDelay          </td><td>arrival delay, in minutes</td>\n</tr><tr>\n <td>16 </td><td> DepDelay          </td><td>departure delay, in minutes</td>\n</tr><tr>\n <td>17 </td><td> Origin            </td><td>origin <a href=\"supplemental-data.html\">IATA airport code</a></td>\n</tr><tr>\n <td>18 </td><td> Dest              </td><td>destination <a href=\"supplemental-data.html\">IATA airport code</a></td>\n</tr><tr>\n <td>19 </td><td> Distance          </td><td>in miles</td>\n</tr><tr>\n <td>20 </td><td> TaxiIn            </td><td>taxi in time, in minutes</td>\n</tr><tr>\n <td>21 </td><td> TaxiOut           </td><td>taxi out time in minutes</td>\n</tr><tr>\n <td>22 </td><td> Cancelled           </td><td>was the flight cancelled?</td>\n</tr><tr>\n <td>23 </td><td> CancellationCode  </td><td>reason for cancellation (A = carrier, B = weather, C = NAS, D = security)</td>\n</tr><tr>\n <td>24 </td><td> Diverted          </td><td>1 = yes, 0 = no</td>\n</tr><tr>\n <td>25 </td><td> CarrierDelay      </td><td>in minutes</td>\n</tr><tr>\n <td>26 </td><td> WeatherDelay      </td><td>in minutes</td>\n</tr><tr>\n <td>27 </td><td> NASDelay          </td><td>in minutes</td>\n</tr><tr>\n <td>28 </td><td> SecurityDelay     </td><td>in minutes</td>\n</tr><tr>\n <td>29 </td><td> LateAircraftDelay </td><td>in minutes</td>\n</tr>\n</tbody></table>",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298135_-1858081320",
      "id":"20160410-003138_1626463388",
      "result":{
        "code":"SUCCESS",
        "type":"ANGULAR",
        "msg":"<table width=\"100%\">\n<tbody><tr>\n  <th></th>\n  <th>Name</th>\n  <th>Description</th>\n</tr>\n<tr>\n <td>1  </td><td> Year              </td><td>1987-2008</td>\n</tr><tr>\n <td>2  </td><td> Month             </td><td>1-12</td>\n</tr><tr>\n <td>3  </td><td> DayofMonth        </td><td>1-31</td>\n</tr><tr>\n <td>4  </td><td> DayOfWeek         </td><td>1 (Monday) - 7 (Sunday)</td>\n</tr><tr>\n <td>5  </td><td> DepTime           </td><td>actual departure time (local, hhmm)</td>\n</tr><tr>\n <td>6  </td><td> CRSDepTime        </td><td>scheduled departure time (local, hhmm)</td>\n</tr><tr>\n <td>7  </td><td> ArrTime           </td><td>actual arrival time (local, hhmm)</td>\n</tr><tr>\n <td>8  </td><td> CRSArrTime        </td><td>scheduled arrival time (local, hhmm)</td>\n</tr><tr>\n <td>9  </td><td> UniqueCarrier     </td><td><a href=\"supplemental-data.html\">unique carrier code</a></td>\n</tr><tr>\n <td>10 </td><td> FlightNum         </td><td>flight number</td>\n</tr><tr>\n <td>11 </td><td> TailNum           </td><td>plane tail number</td>\n</tr><tr>\n <td>12 </td><td> ActualElapsedTime </td><td>in minutes</td>\n</tr><tr>\n <td>13 </td><td> CRSElapsedTime    </td><td>in minutes</td>\n</tr><tr>\n <td>14 </td><td> AirTime           </td><td>in minutes</td>\n</tr><tr>\n <td>15 </td><td> ArrDelay          </td><td>arrival delay, in minutes</td>\n</tr><tr>\n <td>16 </td><td> DepDelay          </td><td>departure delay, in minutes</td>\n</tr><tr>\n <td>17 </td><td> Origin            </td><td>origin <a href=\"supplemental-data.html\">IATA airport code</a></td>\n</tr><tr>\n <td>18 </td><td> Dest              </td><td>destination <a href=\"supplemental-data.html\">IATA airport code</a></td>\n</tr><tr>\n <td>19 </td><td> Distance          </td><td>in miles</td>\n</tr><tr>\n <td>20 </td><td> TaxiIn            </td><td>taxi in time, in minutes</td>\n</tr><tr>\n <td>21 </td><td> TaxiOut           </td><td>taxi out time in minutes</td>\n</tr><tr>\n <td>22 </td><td> Cancelled           </td><td>was the flight cancelled?</td>\n</tr><tr>\n <td>23 </td><td> CancellationCode  </td><td>reason for cancellation (A = carrier, B = weather, C = NAS, D = security)</td>\n</tr><tr>\n <td>24 </td><td> Diverted          </td><td>1 = yes, 0 = no</td>\n</tr><tr>\n <td>25 </td><td> CarrierDelay      </td><td>in minutes</td>\n</tr><tr>\n <td>26 </td><td> WeatherDelay      </td><td>in minutes</td>\n</tr><tr>\n <td>27 </td><td> NASDelay          </td><td>in minutes</td>\n</tr><tr>\n <td>28 </td><td> SecurityDelay     </td><td>in minutes</td>\n</tr><tr>\n <td>29 </td><td> LateAircraftDelay </td><td>in minutes</td>\n</tr>\n</tbody></table>"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:69"
    },
    {
      "text":"%md\n### Part 1: Using DataFrames API to Analyze Airline Dataset",
      "dateUpdated":"Apr 10, 2016 10:52:31 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298135_-1858081320",
      "id":"20160410-003138_650819453",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<h3>Part 1: Using DataFrames API to Analyze Airline Dataset</h3>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "dateStarted":"Apr 10, 2016 10:52:16 AM",
      "dateFinished":"Apr 10, 2016 10:52:16 AM",
      "status":"FINISHED",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:70"
    },
    {
      "title":"Part 1: Using DataFrames to Analyze Dataset",
      "text":"// Show a subset of columns with \"select\"\ndf.select(\"UniqueCarrier\", \"FlightNum\", \"DepDelay\", \"ArrDelay\", \"Distance\").show",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":false,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298135_-1858081320",
      "id":"20160410-003138_1188332400",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:71"
    },
    {
      "text":"// Create a DataFrame containing Flights with delayed Departure by more than 15 min using \"filter\"\nval delayedDF = df.select(\"UniqueCarrier\", \"DepDelay\").filter($\"DepDelay\" > 15).cache\ndelayedDF.show",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298135_-1858081320",
      "id":"20160410-003138_704729700",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:72"
    },
    {
      "text":"// Print total number of delayed flights\nprintln(\"Total Number of Delayed Flights: \" + delayedDF.count)",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298135_-1858081320",
      "id":"20160410-003138_1019754695",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:73"
    },
    {
      "title":"Define a UDF to Determine Delays",
      "text":"import org.apache.spark.sql.functions.udf\n\n// Define a UDF to find delayed flights\n\n// Assume:\n//  if ArrDelay is not available then Delayed = False\n//  if ArrDelay > 15 min then Delayed = True else False\n\nval isDelayedUDF = udf((time: String) => if (time == \"NA\") 0 else if (time.toInt > 15) 1 else 0)",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298135_-1858081320",
      "id":"20160410-003138_2097655805",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:74"
    },
    {
      "text":"// Define a new DataFrame that contains a subset of the original columns and a new column \"IsDelayed\" by applying a UDF\n// isDelayed()  on \"DepDelay\" column\n\nval updatedDF = df.select($\"Year\", $\"Month\", $\"DayofMonth\", $\"DayOfWeek\", $\"CRSDepTime\", $\"UniqueCarrier\", $\"FlightNum\", \n                    $\"DepDelay\", $\"Origin\", $\"Dest\", $\"TaxiIn\", $\"TaxiOut\", $\"Distance\",\n                    isDelayedUDF($\"DepDelay\").alias(\"IsDelayed\")).cache\n\nupdatedDF.show // Notice new column \"IsDelayed\"",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298135_-1858081320",
      "id":"20160410-003138_62081836",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:75"
    },
    {
      "title":"Calculate Percentage of Delayed Flights",
      "text":"updatedDF.agg((sum(\"IsDelayed\") * 100 / count(\"DepDelay\")).alias(\"Percentage of Delayed Flights\")).show",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298136_-1860005065",
      "id":"20160410-003138_1879848857",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:76"
    },
    {
      "title":"Find Avg Taxi-out",
      "text":"// Show only Origin, Dest, and TaxiOut columns\nupdatedDF.select(\"Origin\", \"Dest\", \"TaxiOut\").groupBy(\"Origin\", \"Dest\").agg(avg(\"TaxiOut\").alias(\"AvgTaxiOut\")).orderBy(desc(\"AvgTaxiOut\")).show(10)",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298136_-1860005065",
      "id":"20160410-003138_840324935",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:77"
    },
    {
      "text":"%md\n\nIn the next paragraph replace `<FILL IN>` with your own code. Reference [SparkSQL documentation](https://spark.apache.org/docs/1.1.0/sql-programming-guide.html) and previous examples.",
      "dateUpdated":"Apr 10, 2016 12:38:52 AM",
      "config":{
        "colWidth":12,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "enabled":true,
        "editorMode":"ace/mode/markdown",
        "editorHide":true
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248464619_1041079907",
      "id":"20160410-003424_739062063",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<p>In the next paragraph replace <code>&lt;FILL IN&gt;</code> with your own code. Reference <a href=\"https://spark.apache.org/docs/1.1.0/sql-programming-guide.html\">SparkSQL documentation</a> and previous examples.</p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:34:24 AM",
      "dateStarted":"Apr 10, 2016 12:38:50 AM",
      "dateFinished":"Apr 10, 2016 12:38:51 AM",
      "status":"FINISHED",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:78"
    },
    {
      "title":"Find Avg Taxi-in",
      "text":"// Find average TaxiIn\n// Show only Origin, Dest, and TaxiIn columns\n// Use aggregate functions on the updatedDF DataFrame to complete the code\n\nupdatedDF.<FILL IN>",
      "dateUpdated":"Apr 10, 2016 12:37:07 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298136_-1860005065",
      "id":"20160410-003138_1488719873",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:79"
    },
    {
      "title":"SOLUTION (click 'Show output', i.e. closed notepad icon on the right)",
      "text":"%md\n\n`updatedDF.select(\"Origin\", \"Dest\", \"TaxiIn\").groupBy(\"Origin\", \"Dest\").agg(avg(\"TaxiIn\").alias(\"AvgTaxiIn\")).orderBy(desc(\"AvgTaxiIn\")).show(10)`",
      "dateUpdated":"Apr 10, 2016 12:46:17 AM",
      "config":{
        "colWidth":12,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "enabled":true,
        "editorMode":"ace/mode/markdown",
        "title":true,
        "editorHide":true,
        "tableHide":true
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248529430_823170823",
      "id":"20160410-003529_382090485",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<p><code>updatedDF.select(\"Origin\", \"Dest\", \"TaxiIn\").groupBy(\"Origin\", \"Dest\").agg(avg(\"TaxiIn\").alias(\"AvgTaxiIn\")).orderBy(desc(\"AvgTaxiIn\")).show(10)</code></p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:35:29 AM",
      "dateStarted":"Apr 10, 2016 12:37:24 AM",
      "dateFinished":"Apr 10, 2016 12:37:25 AM",
      "status":"FINISHED",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:80"
    },
    {
      "text":"%md\n### Part 2: Using SQL to Analyze the Airline Dataset",
      "dateUpdated":"Apr 10, 2016 10:53:25 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298136_-1860005065",
      "id":"20160410-003138_582934314",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<h3>Part 2: Using SQL to Analyze the Airline Dataset</h3>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "dateStarted":"Apr 10, 2016 10:53:24 AM",
      "dateFinished":"Apr 10, 2016 10:53:25 AM",
      "status":"FINISHED",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:81"
    },
    {
      "title":"",
      "text":"%md\nNow let's use SQL statements to analyze our dataset.",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":false,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298136_-1860005065",
      "id":"20160410-003138_556617784",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<p>Now let's use SQL statements to analyze our dataset.</p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:82"
    },
    {
      "title":"Register a Temporary Table",
      "text":"// Convert DataFrame to a Temporary Table\nupdatedDF.registerTempTable(\"flightsTempTbl\")",
      "dateUpdated":"Apr 10, 2016 12:39:30 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298136_-1860005065",
      "id":"20160410-003138_636329356",
      "result":{
        "code":"SUCCESS",
        "type":"TEXT",
        "msg":""
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "dateStarted":"Apr 10, 2016 12:39:22 AM",
      "dateFinished":"Apr 10, 2016 12:39:22 AM",
      "status":"FINISHED",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:83"
    },
    {
      "title":"Preview Table",
      "text":"%sql\n\nSELECT * FROM flightsTempTbl LIMIT 10",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/sql",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298137_-1860389814",
      "id":"20160410-003138_318924232",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:84"
    },
    {
      "title":"Register UDF",
      "text":"// Register a UDF to find delayed flights\n// Note that this is a UDF specific for use within the sqlContext\n\n// Assume:\n//  if ArrDelay is not available then Delayed = False\n//  if ArrDelay > 15 min then Delayed = True else False\n\nsqlContext.udf.register(\"isDelayedUDF\", (time: String) => if (time == \"NA\") 0 else if (time.toInt > 15) 1 else 0)",
      "dateUpdated":"Apr 10, 2016 12:40:51 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298137_-1860389814",
      "id":"20160410-003138_40384312",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:85"
    },
    {
      "title":"Compare Total Number of Delayed Flights by Carrier",
      "text":"%sql\n\nSELECT UniqueCarrier, SUM(isDelayedUDF(DepDelay)) AS NumDelays FROM flightsTempTbl GROUP BY UniqueCarrier",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"pieChart",
          "height":296,
          "optionOpen":false,
          "keys":[
            {
              "name":"UniqueCarrier",
              "index":0,
              "aggr":"sum"
            }
          ],
          "values":[
            {
              "name":"NumDelays",
              "index":1,
              "aggr":"sum"
            }
          ],
          "groups":[

          ],
          "scatter":{
            "yAxis":{
              "name":"NumDelays",
              "index":1,
              "aggr":"sum"
            }
          }
        },
        "editorMode":"ace/mode/sql",
        "colWidth":6
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298137_-1860389814",
      "id":"20160410-003138_134299332",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:86"
    },
    {
      "title":"Compare Total Delayed Time (min) by Carrier",
      "text":"%sql\n\nSELECT UniqueCarrier, COUNT(DepDelay) AS TotalTimeDelay FROM flightsTempTbl GROUP BY UniqueCarrier",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"pieChart",
          "height":300,
          "optionOpen":false,
          "keys":[
            {
              "name":"UniqueCarrier",
              "index":0,
              "aggr":"sum"
            }
          ],
          "values":[
            {
              "name":"TotalTimeDelay",
              "index":1,
              "aggr":"sum"
            }
          ],
          "groups":[

          ],
          "scatter":{
            "xAxis":{
              "name":"UniqueCarrier",
              "index":0,
              "aggr":"sum"
            },
            "yAxis":{
              "name":"TotalTimeDelay",
              "index":1,
              "aggr":"sum"
            }
          }
        },
        "editorMode":"ace/mode/sql",
        "colWidth":6
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298137_-1860389814",
      "id":"20160410-003138_163559927",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:87"
    },
    {
      "text":"%md\n\nIn the paragraph below replace `<FILL IN>` with your code.",
      "dateUpdated":"Apr 10, 2016 12:43:01 AM",
      "config":{
        "colWidth":12,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "enabled":true,
        "editorMode":"ace/mode/markdown",
        "editorHide":true
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248881653_1928860897",
      "id":"20160410-004121_2117120539",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<p>In the paragraph below replace <code>&lt;FILL IN&gt;</code> with your code.</p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:41:21 AM",
      "dateStarted":"Apr 10, 2016 12:43:00 AM",
      "dateFinished":"Apr 10, 2016 12:43:00 AM",
      "status":"FINISHED",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:88"
    },
    {
      "title":"Find Average Distance Travelled by Carrier",
      "text":"%sql\n\n-- Find average distance by UniqueCarrier from flightsTempTbl\n-- order descending by average distance\n\nSELECT <FILL IN>",
      "dateUpdated":"Apr 10, 2016 10:54:02 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/sql",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298137_-1860389814",
      "id":"20160410-003138_172624929",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:89"
    },
    {
      "title":"SOLUTION (click 'Show output', i.e. closed notepad icon on the right)",
      "text":"%md\n\n`UniqueCarrier, avg(Distance) AS AvgDistanceTraveled FROM flightsTempTbl GROUP BY UniqueCarrier ORDER BY AvgDistanceTraveled DESC`",
      "dateUpdated":"Apr 10, 2016 12:46:23 AM",
      "config":{
        "colWidth":12,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "enabled":true,
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "title":true,
        "tableHide":true
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460249108342_-53271323",
      "id":"20160410-004508_2129481983",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<p><code>UniqueCarrier, avg(Distance) AS AvgDistanceTraveled FROM flightsTempTbl GROUP BY UniqueCarrier ORDER BY AvgDistanceTraveled DESC</code></p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:45:08 AM",
      "dateStarted":"Apr 10, 2016 12:45:17 AM",
      "dateFinished":"Apr 10, 2016 12:45:18 AM",
      "status":"FINISHED",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:90"
    },
    {
      "title":"Find Out When Most Flights Get Delayed by Day of Week",
      "text":"%sql\n\nSELECT DayOfWeek, CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END AS Delay, COUNT(1) AS Count\nFROM flightsTempTbl\nGROUP BY DayOfWeek, CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END\nORDER BY DayOfWeek",
      "dateUpdated":"Apr 10, 2016 12:48:03 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"multiBarChart",
          "height":300,
          "optionOpen":false,
          "keys":[
            {
              "name":"DayOfWeek",
              "index":0,
              "aggr":"sum"
            }
          ],
          "values":[
            {
              "name":"Count",
              "index":2,
              "aggr":"sum"
            }
          ],
          "groups":[
            {
              "name":"Delay",
              "index":1,
              "aggr":"sum"
            }
          ],
          "scatter":{
            "xAxis":{
              "name":"DayOfWeek",
              "index":0,
              "aggr":"sum"
            },
            "yAxis":{
              "name":"Delay",
              "index":1,
              "aggr":"sum"
            }
          }
        },
        "editorMode":"ace/mode/sql",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298137_-1860389814",
      "id":"20160410-003138_56774606",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:91"
    },
    {
      "title":"Find Out When Most Flights Get Delayed by Hour",
      "text":"%sql\n\nSELECT CAST(CRSDepTime / 100 AS INT) AS Hour, CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END AS Delay, COUNT(1) AS Count\nFROM flightsTempTbl\nGROUP BY CAST(CRSDepTime / 100 AS INT), CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END\nORDER BY Hour",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"multiBarChart",
          "height":300,
          "optionOpen":false,
          "keys":[
            {
              "name":"Hour",
              "index":0,
              "aggr":"sum"
            }
          ],
          "values":[
            {
              "name":"Count",
              "index":2,
              "aggr":"sum"
            }
          ],
          "groups":[
            {
              "name":"Delay",
              "index":1,
              "aggr":"sum"
            }
          ],
          "scatter":{
            "xAxis":{
              "name":"Hour",
              "index":0,
              "aggr":"sum"
            },
            "yAxis":{
              "name":"Delay",
              "index":1,
              "aggr":"sum"
            }
          }
        },
        "editorMode":"ace/mode/sql",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298137_-1860389814",
      "id":"20160410-003138_728063774",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:92"
    },
    {
      "text":"%angular\n\n<h3>Save Modes</h3>\n\nSave operations can optionally take a <code>SaveMode</code>, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing a <code>Overwrite</code>, the data will be deleted before writing out the new data.\n<br><br>\n<style>\ntable, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n}\nth, td {\n    padding: 5px;\n}\n</style>\n\n<table style=\"width:100%\">\n  <tr>\n    <th>Mode (Scala/Java)</th>\n    <th>Meaning</th>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.ErrorIfExists (default)</code></td>\n    <td>When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td>\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Append</code></td>\n    <td>When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.</td>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Overwrite</code></td>\n    <td>Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Ignore</code></td>\n    <td>Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.</td>\n  </tr>\n</table>",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298138_-1859235567",
      "id":"20160410-003138_206029012",
      "result":{
        "code":"SUCCESS",
        "type":"ANGULAR",
        "msg":"<h3>Save Modes</h3>\n\nSave operations can optionally take a <code>SaveMode</code>, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing a <code>Overwrite</code>, the data will be deleted before writing out the new data.\n<br><br>\n<style>\ntable, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n}\nth, td {\n    padding: 5px;\n}\n</style>\n\n<table style=\"width:100%\">\n  <tr>\n    <th>Mode (Scala/Java)</th>\n    <th>Meaning</th>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.ErrorIfExists (default)</code></td>\n    <td>When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td>\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Append</code></td>\n    <td>When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.</td>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Overwrite</code></td>\n    <td>Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Ignore</code></td>\n    <td>Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.</td>\n  </tr>\n</table>"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:93"
    },
    {
      "title":"Save to ORC file",
      "text":"import org.apache.spark.sql.SaveMode\n\n// Save and Overwrite results to an ORC file\nupdatedDF.write.format(\"orc\").mode(SaveMode.Overwrite).save(\"flightsAndDelays.orc\")",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298138_-1859235567",
      "id":"20160410-003138_985965720",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:94"
    },
    {
      "title":"Load from ORC file",
      "text":"// Load results back from ORC file\nval dfTest = sqlContext.read.format(\"orc\").load(\"flightsAndDelays.orc\")\ndfTest.show",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298138_-1859235567",
      "id":"20160410-003138_1142035788",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:95"
    },
    {
      "title":"Compare DataFrame Sizes (i.e. Compare Original DataFrame with one Loaded from HDFS)",
      "text":"// Note: if output assertion succeeds no warning messages will be printed\nassert (dfTest.count == updatedDF.count, println(\"Assertion Fail: Files are of different sizes.\"))",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298138_-1859235567",
      "id":"20160410-003138_2134135677",
      "result":{
        "code":"SUCCESS",
        "type":"TEXT",
        "msg":""
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:96"
    },
    {
      "text":"%md\n\n### Saving to Persistent Tables\nUnlike the `registerTempTable` command, `saveAsTable` will materialize the contents of the dataframe and create a pointer to the data in the HiveMetastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the `table` method on a `SQLContext` with the name of the table.<br>\nBy default `saveAsTable` will create a “managed table”, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped.<br>",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298138_-1859235567",
      "id":"20160410-003138_1146451145",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<h3>Saving to Persistent Tables</h3>\n<p>Unlike the <code>registerTempTable</code> command, <code>saveAsTable</code> will materialize the contents of the dataframe and create a pointer to the data in the HiveMetastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the <code>table</code> method on a <code>SQLContext</code> with the name of the table.<br>\n<br  />By default <code>saveAsTable</code> will create a “managed table”, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped.<br></p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:97"
    },
    {
      "title":"Save to a Table",
      "text":"updatedDF.write.format(\"orc\").mode(SaveMode.Overwrite).saveAsTable(\"flightsPermTbl\")",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298138_-1859235567",
      "id":"20160410-003138_1181113131",
      "result":{
        "code":"SUCCESS",
        "type":"TEXT",
        "msg":""
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:98"
    },
    {
      "title":"Read from Table to a New DataFrame",
      "text":"val dfFromTbl = sqlContext.table(\"flightsPermTbl\")",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298138_-1859235567",
      "id":"20160410-003138_1777043496",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:99"
    },
    {
      "title":"Show Elements of DataFrame",
      "text":"dfFromTbl.show(5)",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298139_-1859620316",
      "id":"20160410-003138_1714958443",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:100"
    },
    {
      "title":"Show Tables",
      "text":"%sql\n\nSHOW Tables\n\n-- Notice that unlike flightsTempTbl, flightsPermTbl is a permanent table",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[
            {
              "name":"tableName",
              "index":0,
              "aggr":"sum"
            }
          ],
          "values":[
            {
              "name":"isTemporary",
              "index":1,
              "aggr":"sum"
            }
          ],
          "groups":[

          ],
          "scatter":{
            "xAxis":{
              "name":"tableName",
              "index":0,
              "aggr":"sum"
            },
            "yAxis":{
              "name":"isTemporary",
              "index":1,
              "aggr":"sum"
            }
          }
        },
        "editorMode":"ace/mode/sql",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298139_-1859620316",
      "id":"20160410-003138_1928525114",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:101"
    },
    {
      "title":"Drop Managed Table",
      "text":"%sql\n\nDROP TABLE flightsPermTbl",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298139_-1859620316",
      "id":"20160410-003138_750906827",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:102"
    },
    {
      "title":"Describe Table",
      "text":"%sql\n\nSHOW tables\n\n-- notice that flightsPermTbl is no longer available",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[
            {
              "name":"tableName",
              "index":0,
              "aggr":"sum"
            }
          ],
          "values":[
            {
              "name":"isTemporary",
              "index":1,
              "aggr":"sum"
            }
          ],
          "groups":[

          ],
          "scatter":{
            "xAxis":{
              "name":"tableName",
              "index":0,
              "aggr":"sum"
            },
            "yAxis":{
              "name":"isTemporary",
              "index":1,
              "aggr":"sum"
            }
          }
        },
        "editorMode":"ace/mode/sql",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298139_-1859620316",
      "id":"20160410-003138_1877131617",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:103"
    },
    {
      "title":"The End",
      "text":"%md\nYou've reached the end of this lab! We hope you've been able to successfully complete all portions of this lab.",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298139_-1859620316",
      "id":"20160410-003138_268828842",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<p>You've reached the end of this lab! We hope you've been able to successfully complete all portions of this lab.</p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:104"
    },
    {
      "text":"%md\n### Additional Resources\nThis is just the beggining of your journey with Spark. Make sure to checkout these additional useful resources:\n\n1. [Hortonworks Community Connection](https://hortonworks.com/community/) (HCC) for guidance, code, examples and best practices to jump start your projects.\n2. [Spark SQL, DataFrames and DataSets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n3. [A Lap Around Spark](http://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/)",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "tableHide":false,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298139_-1859620316",
      "id":"20160410-003138_2048237853",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<h3>Additional Resources</h3>\n<p>This is just the beggining of your journey with Spark. Make sure to checkout these additional useful resources:</p>\n<ol>\n<li><a href=\"https://hortonworks.com/community/\">Hortonworks Community Connection</a> (HCC) for guidance, code, examples and best practices to jump start your projects.</li>\n<li><a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\">Spark SQL, DataFrames and DataSets Guide</a></li>\n<li><a href=\"http://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/\">A Lap Around Spark</a></li>\n</ol>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:105"
    },
    {
      "title":"Feedback",
      "text":"%md\nPlease let us know what you thought of this lab. Your feedback will be incorporated to improve this lab for our future meetups.\n#\n**[Give Feedback](http://hor.tn/mylabfeedback)**\n#\nThank you!",
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "title":true,
        "tableHide":false,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/markdown",
        "editorHide":true,
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298139_-1859620316",
      "id":"20160410-003138_570733961",
      "result":{
        "code":"SUCCESS",
        "type":"HTML",
        "msg":"<p>Please let us know what you thought of this lab. Your feedback will be incorporated to improve this lab for our future meetups.</p>\n<h1></h1>\n<p><strong><a href=\"http://hor.tn/mylabfeedback\">Give Feedback</a></strong></p>\n<h1></h1>\n<p>Thank you!</p>\n"
      },
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:106"
    },
    {
      "dateUpdated":"Apr 10, 2016 12:31:38 AM",
      "config":{
        "enabled":true,
        "graph":{
          "mode":"table",
          "height":300,
          "optionOpen":false,
          "keys":[

          ],
          "values":[

          ],
          "groups":[

          ],
          "scatter":{

          }
        },
        "editorMode":"ace/mode/scala",
        "colWidth":12
      },
      "settings":{
        "params":{

        },
        "forms":{

        }
      },
      "jobName":"paragraph_1460248298140_-1861544060",
      "id":"20160410-003138_1663715025",
      "dateCreated":"Apr 10, 2016 12:31:38 AM",
      "status":"READY",
      "errorMessage":"",
      "progressUpdateIntervalMs":500,
      "$$hashKey":"object:107"
    }
  ],
  "name":"Lab 102: Intro to Spark SQL and DataFrames with Scala",
  "id":"2BJVW65WS",
  "angularObjects":{
    "2BF4FYFEE":[

    ],
    "2BEK3QEPR":[

    ],
    "2BEUGKP8A":[

    ],
    "2BED2XA8B":[

    ],
    "2BFC382WH":[

    ],
    "2BEDYW971":[

    ],
    "2BGY6N3UD":[

    ],
    "2BENYVD9X":[

    ],
    "2BGFTGT6B":[

    ],
    "2BFYENWB4":[

    ],
    "2BFDPRBCM":[

    ],
    "2BDKJZJ55":[

    ],
    "2BFUWR97N":[

    ]
  },
  "config":{
    "looknfeel":"default"
  },
  "info":{

  }
}