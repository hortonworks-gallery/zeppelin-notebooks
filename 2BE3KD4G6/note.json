{"paragraphs":[{"text":"%md\n\n# Intro to Spark with Python\n#### RDDs, DataFrames, and Spark Streaming\nby Robert Hryniewicz\nver 0.8\n","dateUpdated":"Mar 22, 2016 8:10:37 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189997_1750617357","id":"20160322-194949_664190205","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Intro to Spark with Python</h1>\n<h4>RDDs, DataFrames, and Spark Streaming</h4>\n<p>by Robert Hryniewicz\n<br  />ver 0.8</p>\n"},"dateCreated":"Mar 22, 2016 7:49:49 PM","dateStarted":"Mar 22, 2016 8:10:32 PM","dateFinished":"Mar 22, 2016 8:10:32 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:74"},{"text":"%md\n## Introduction\n\nThis lab consists of three parts. In each section you will perform a basic Word Count.\n#\nIn Part 1, we will introduce RDDs, Spark's primary low-level abstraction, and several core concepts.\n#\nIn Part 2, we will introduce DataFrames, a higher-level abstraction than RDDs, along with SparkSQL allowing you to use SQL statements to query a temporary table.\n#\nIn Part 3, we will introduce Spark Streaming, allowing you to compute near-realtime data using micro-batches. This is useful for data in motion. In this part you will use *Shell in a Browser* to run a Spark Streaming job.","dateUpdated":"Mar 22, 2016 7:58:06 PM","config":{"enabled":true,"graph":{"mode":"table","height":217,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189997_1750617357","id":"20160322-194949_424778329","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Introduction</h2>\n<p>This lab consists of three parts. In each section you will perform a basic Word Count.</p>\n<h1></h1>\n<p>In Part 1, we will introduce RDDs, Spark's primary low-level abstraction, and several core concepts.</p>\n<h1></h1>\n<p>In Part 2, we will introduce DataFrames, a higher-level abstraction than RDDs, along with SparkSQL allowing you to use SQL statements to query a temporary table.</p>\n<h1></h1>\n<p>In Part 3, we will introduce Spark Streaming, allowing you to compute near-realtime data using micro-batches. This is useful for data in motion. In this part you will use <em>Shell in a Browser</em> to run a Spark Streaming job.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:75"},{"text":"%md\n### Concepts\n\nAt the core of Spark is the notion of a Resilient Distributed Dataset (RDD), which is an immutable and fault-tolerant collection of objects that is partitioned and distributed across multiple physical nodes of a YARN cluster that can be operated in parallel.\n#\nTypically, RDDs are instantiated by loading data from a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat on a YARN cluster.\n#\nOnce an RDD is instantiated, you can apply a [series of operations](https://spark.apache.org/docs/latest/programming-guide.html#rdd-operations). All operations fall into one of two types: **[Transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations)** or **[Actions](https://spark.apache.org/docs/latest/programming-guide.html#actions)**. Transformation operations, as the name suggests, create new datasets from an existing RDD and build out the processing Directed Acyclic Graph (DAG) that can then be applied on the partitioned dataset across the YARN cluster. An Action operation, on the other hand, executes DAG and returns a value.\n#\nIn this lab we will use the following **Transformations**:\n- map(func)\n- filter(func)\n- flatMap(func)\n- reduceByKey(func)\n\nand **Actions**:\n\n- collect()\n- count()\n- take()\n- takeOrdered(n, [ordering])\n- countByKey()\n\nA typical Spark application has the following four phases:\n\n```\n┏━━━━━━━━━━━━━━━━━━━━━━━┓\n┃                       ┃\n┃      Instantiate      ┃\n┃      Input RDDs       ┃\n┃                       ┃\n┗━━━━━━━━━━━┳━━━━━━━━━━━┛\n            │\n            │\n┏━━━━━━━━━━━▼━━━━━━━━━━━┓\n┃                       ┃\n┃    Transform RDDs     ┃\n┃                       ┃\n┃                       ┃\n┗━━━━━━━━━━━┳━━━━━━━━━━━┛\n            │\n            │\n┏━━━━━━━━━━━▼━━━━━━━━━━━┓\n┃                       ┃\n┃        Persist        ┃\n┃   Intermediate RDDs   ┃\n┃                       ┃\n┗━━━━━━━━━━━┳━━━━━━━━━━━┛\n            │\n            │\n┏━━━━━━━━━━━▼━━━━━━━━━━━┓\n┃                       ┃\n┃    Action on RDDs     ┃\n┃                       ┃\n┃                       ┃\n┗━━━━━━━━━━━━━━━━━━━━━━━┛\n```","dateUpdated":"Mar 22, 2016 8:20:08 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189997_1750617357","id":"20160322-194949_1013732875","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Concepts</h3>\n<p>At the core of Spark is the notion of a Resilient Distributed Dataset (RDD), which is an immutable and fault-tolerant collection of objects that is partitioned and distributed across multiple physical nodes of a YARN cluster that can be operated in parallel.</p>\n<h1></h1>\n<p>Typically, RDDs are instantiated by loading data from a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat on a YARN cluster.</p>\n<h1></h1>\n<p>Once an RDD is instantiated, you can apply a <a href=\"https://spark.apache.org/docs/latest/programming-guide.html#rdd-operations\">series of operations</a>. All operations fall into one of two types: <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#transformations\">Transformations</a></strong> or <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#actions\">Actions</a></strong>. Transformation operations, as the name suggests, create new datasets from an existing RDD and build out the processing Directed Acyclic Graph (DAG) that can then be applied on the partitioned dataset across the YARN cluster. An Action operation, on the other hand, executes DAG and returns a value.</p>\n<h1></h1>\n<p>In this lab we will use the following <strong>Transformations</strong>:</p>\n<ul>\n<li>map(func)</li>\n<li>filter(func)</li>\n<li>flatMap(func)</li>\n<li>reduceByKey(func)</li>\n</ul>\n<p>and <strong>Actions</strong>:</p>\n<ul>\n<li>collect()</li>\n<li>count()</li>\n<li>take()</li>\n<li>takeOrdered(n, [ordering])</li>\n<li>countByKey()</li>\n</ul>\n<p>A typical Spark application has the following four phases:</p>\n<pre><code>┏━━━━━━━━━━━━━━━━━━━━━━━┓\n┃                       ┃\n┃      Instantiate      ┃\n┃      Input RDDs       ┃\n┃                       ┃\n┗━━━━━━━━━━━┳━━━━━━━━━━━┛\n            │\n            │\n┏━━━━━━━━━━━▼━━━━━━━━━━━┓\n┃                       ┃\n┃    Transform RDDs     ┃\n┃                       ┃\n┃                       ┃\n┗━━━━━━━━━━━┳━━━━━━━━━━━┛\n            │\n            │\n┏━━━━━━━━━━━▼━━━━━━━━━━━┓\n┃                       ┃\n┃        Persist        ┃\n┃   Intermediate RDDs   ┃\n┃                       ┃\n┗━━━━━━━━━━━┳━━━━━━━━━━━┛\n            │\n            │\n┏━━━━━━━━━━━▼━━━━━━━━━━━┓\n┃                       ┃\n┃    Action on RDDs     ┃\n┃                       ┃\n┃                       ┃\n┗━━━━━━━━━━━━━━━━━━━━━━━┛\n</code></pre>\n"},"dateCreated":"Mar 22, 2016 7:49:49 PM","dateStarted":"Mar 22, 2016 8:20:07 PM","dateFinished":"Mar 22, 2016 8:20:07 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:76"},{"text":"%md\n### Lab Pre-Check\nBefore we proceed let's take look at Spark Version and Spark Cofiguration information.","dateUpdated":"Mar 22, 2016 7:49:49 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189997_1750617357","id":"20160322-194949_1820658474","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Lab Pre-Check</h3>\n<p>Before we proceed let's take look at Spark Version and Spark Cofiguration information.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:77"},{"text":"%md\n#\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button on the right-hand side of each paragraph or simply press `Shift + Enter`.","dateUpdated":"Mar 22, 2016 7:49:49 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189998_1751771603","id":"20160322-194949_312486415","result":{"code":"SUCCESS","type":"HTML","msg":"<h1></h1>\n<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button on the right-hand side of each paragraph or simply press <code>Shift + Enter</code>.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:78"},{"title":"Check Spark Version","text":"sc.version","dateUpdated":"Mar 22, 2016 8:01:28 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189998_1751771603","id":"20160322-194949_1361659990","dateCreated":"Mar 22, 2016 7:49:49 PM","dateStarted":"Mar 22, 2016 8:01:28 PM","dateFinished":"Mar 22, 2016 8:01:29 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:79"},{"title":"Check Spark Configuration","text":"sc.getConf.toDebugString.foreach(print)","dateUpdated":"Mar 22, 2016 7:57:50 PM","config":{"enabled":true,"title":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189998_1751771603","id":"20160322-194949_1833709498","dateCreated":"Mar 22, 2016 7:49:49 PM","dateStarted":"Mar 22, 2016 7:57:50 PM","dateFinished":"Mar 22, 2016 7:58:42 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:80"},{"title":"Take a Quick Look at Overall Available Resources","text":"%sh free -mh","dateUpdated":"Mar 22, 2016 8:01:41 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sh","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189998_1751771603","id":"20160322-194949_617714789","dateCreated":"Mar 22, 2016 7:49:49 PM","dateStarted":"Mar 22, 2016 8:01:41 PM","dateFinished":"Mar 22, 2016 8:01:42 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:81"},{"text":"%md ####Now let's proceed with our core lab.\n","dateUpdated":"Mar 22, 2016 7:49:49 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189998_1751771603","id":"20160322-194949_996509125","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Now let's proceed with our core lab.</h4>\n"},"dateCreated":"Mar 22, 2016 7:49:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:82"},{"text":"%md \n\n## Part 1\n#### Introduction to RDDs with Word Count example","dateUpdated":"Mar 22, 2016 7:49:49 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189999_1751386855","id":"20160322-194949_103900922","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Part 1</h2>\n<h4>Introduction to RDDs with Word Count example</h4>\n"},"dateCreated":"Mar 22, 2016 7:49:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:83"},{"text":"%md\nIn this section you will perform a basic word count with RDDs.\n#\nYou will download external text data file to your sandbox. Then you will perform lexical analysis, or tokenization, by breaking up text into words/tokens.\nThe list of tokens then becomes an input for further processing to this and following sections.\n#\nBy the end of this section you should have learned how to perform low-level transformations and actions with Spark RDDs and lambda expressions.","dateUpdated":"Mar 22, 2016 7:49:49 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189999_1751386855","id":"20160322-194949_358591043","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In this section you will perform a basic word count with RDDs.</p>\n<h1></h1>\n<p>You will download external text data file to your sandbox. Then you will perform lexical analysis, or tokenization, by breaking up text into words/tokens.\n<br  />The list of tokens then becomes an input for further processing to this and following sections.</p>\n<h1></h1>\n<p>By the end of this section you should have learned how to perform low-level transformations and actions with Spark RDDs and lambda expressions.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:84"},{"text":"%md\nIn the next paragraph we are going to download data using shell commands. \nA shell command in a Zeppelin notebook can be invoked by prepending a block of shell commands with a line containing `%sh` characters.","dateUpdated":"Mar 22, 2016 7:49:49 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189999_1751386855","id":"20160322-194949_1000546219","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In the next paragraph we are going to download data using shell commands.\n<br  />A shell command in a Zeppelin notebook can be invoked by prepending a block of shell commands with a line containing <code>%sh</code> characters.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:49 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85"},{"title":"Prepare Directories and Download a Dataset","text":"%sh\ncd /tmp\n\n#  Remove old dataset file if already exists in local /tmp directory\nif [ -e /tmp/About-Apache-NiFi.txt ]\nthen\n    rm -f /tmp/About-Apache-NiFi.txt\nfi\n\n# Remove old dataset if already exists in hadoop /tmp directory\nif hadoop fs -stat /tmp/About-Apache-NiFi.txt\nthen\n   hadoop fs -rm  /tmp/About-Apache-NiFi.txt\nfi\n\n# Download \"About-Apache-NiFi\" text file\nwget https://raw.githubusercontent.com/roberthryniewicz/datasets/master/About-Apache-NiFi.txt\n\n# Move dataset to hadoop /tmp\nhadoop fs -put About-Apache-NiFi.txt /tmp","dateUpdated":"Mar 22, 2016 8:06:52 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sh","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189999_1751386855","id":"20160322-194949_1383346723","dateCreated":"Mar 22, 2016 7:49:49 PM","dateStarted":"Mar 22, 2016 8:06:52 PM","dateFinished":"Mar 22, 2016 8:07:02 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:86"},{"title":"Preview Downloaded Text File","text":"%sh\nhadoop fs -cat /tmp/About-Apache-NiFi.txt | head","dateUpdated":"Mar 22, 2016 8:08:22 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sh","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676189999_1751386855","id":"20160322-194949_1250411039","dateCreated":"Mar 22, 2016 7:49:49 PM","dateStarted":"Mar 22, 2016 8:08:22 PM","dateFinished":"Mar 22, 2016 8:08:25 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:87"},{"text":"%md\nNext we are going to run Spark Python (or PySpark) that can be invoked by prepending a block of Python code with a line containing `%pyspark`.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190000_1761775075","id":"20160322-194950_383362931","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Next we are going to run Spark Python (or PySpark) that can be invoked by prepending a block of Python code with a line containing <code>%pyspark</code>.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:88"},{"text":"%md\n#\nThe important thing to notice in the next paragraph is the `sc` object or Spark Context. Spark Context is automatically created by your driver program in Zeppelin.\n#\nSpark Context is the main entry point for Spark functionality. A Spark Context represents the connection to a Spark cluster, and can be used to create RDDs, which we will do next.\n#\nRemember that Spark doesn't have any storage layer, rather it has connectors to HDFS, S3, Cassandra, HBase, Hive etc. to bring data into memory. Thus, in the next paragraph you will read data (that you've just downloaded) from HDFS.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190000_1761775075","id":"20160322-194950_729840317","result":{"code":"SUCCESS","type":"HTML","msg":"<h1></h1>\n<p>The important thing to notice in the next paragraph is the <code>sc</code> object or Spark Context. Spark Context is automatically created by your driver program in Zeppelin.</p>\n<h1></h1>\n<p>Spark Context is the main entry point for Spark functionality. A Spark Context represents the connection to a Spark cluster, and can be used to create RDDs, which we will do next.</p>\n<h1></h1>\n<p>Remember that Spark doesn't have any storage layer, rather it has connectors to HDFS, S3, Cassandra, HBase, Hive etc. to bring data into memory. Thus, in the next paragraph you will read data (that you've just downloaded) from HDFS.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:89"},{"title":"Read Text File from HDFS and Preview its Contents","text":"%pyspark\n\n# Parallelize text file using pre-initialized Spark context (sc)\nlines = sc.textFile(\"/tmp/About-Apache-NiFi.txt\")\n\n# Take a look at a few lines with a take() action.\nprint lines.take(4)\n\n# Output: Notice that each line has been placed in a seperate array bucket.","dateUpdated":"Mar 22, 2016 8:13:54 PM","config":{"enabled":true,"tableHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190000_1761775075","id":"20160322-194950_2147099802","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:13:55 PM","dateFinished":"Mar 22, 2016 8:13:55 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:90"},{"text":"%md\n#\nIn the next paragraphs we will start using Python lambda (or anonymous) functions. If you're unfamiliar with lambda expressions, review [python docs] (https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions) before proceeding.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190001_1761390326","id":"20160322-194950_2138651673","result":{"code":"SUCCESS","type":"HTML","msg":"<h1></h1>\n<p>In the next paragraphs we will start using Python lambda (or anonymous) functions. If you're unfamiliar with lambda expressions, review <a href=\"https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions\">python docs</a> before proceeding.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:91"},{"title":"Extract All Words from the Document","text":"%pyspark\n# Here we're tokenizing our text file by using the split() function. Each original line of text is split into words or tokens on a single space.\n#  Also, since each line of the original text occupies a seperate bucket in the array, we need to use\n#  a flatMap() transformation to flatten all buckets into a asingle/flat array of tokens.\n\nwords = lines.flatMap(lambda line: line.split(\" \"))","dateUpdated":"Mar 22, 2016 8:20:22 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190001_1761390326","id":"20160322-194950_1884868272","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:20:22 PM","dateFinished":"Mar 22, 2016 8:20:22 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:92"},{"text":"%md\nNote that after you click 'play' in the paragraph above \"nothing\" appears to happen.\n#\nThat's because `flatMap()` is a transformation and all transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently – for example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n#\nBy default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190001_1761390326","id":"20160322-194950_1449770607","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Note that after you click 'play' in the paragraph above &ldquo;nothing&rdquo; appears to happen.</p>\n<h1></h1>\n<p>That's because <code>flatMap()</code> is a transformation and all transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently – for example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.</p>\n<h1></h1>\n<p>By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:93"},{"title":"Take a look at first 100 words","text":"%pyspark\nprint words.take(100)   # we're using a take(n) action\n\n# Output: As you can see, each word occupies a distinc array bucket.","dateUpdated":"Mar 22, 2016 8:20:27 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190001_1761390326","id":"20160322-194950_945132987","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:20:27 PM","dateFinished":"Mar 22, 2016 8:20:28 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:94"},{"text":"%md\n#### Remove Empty Words\n\nUse `filter()` transformation to remove empty words. Remember that in Python the `len()` function returns a length of a string. \n#\nReplace `<FILL IN>` with appropriate code.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190002_1762544573","id":"20160322-194950_2061059297","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Remove Empty Words</h4>\n<p>Use <code>filter()</code> transformation to remove empty words. Remember that in Python the <code>len()</code> function returns a length of a string.</p>\n<h1></h1>\n<p>Replace <code>&lt;FILL IN&gt;</code> with appropriate code.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95"},{"text":"%pyspark\nwordsFiltered = words.filter(<FILL IN>)","dateUpdated":"Mar 22, 2016 10:44:12 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190002_1762544573","id":"20160322-194950_1312378569","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:48:14 PM","dateFinished":"Mar 22, 2016 8:48:15 PM","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:96"},{"title":"Solution (click 'Show output')","text":"%md\n`wordsFiltered = words.filter(lambda w: len(w) > 0)`","dateUpdated":"Mar 23, 2016 7:59:02 PM","config":{"enabled":true,"title":true,"tableHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190002_1762544573","id":"20160322-194950_342070846","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>wordsFiltered = words.filter(lambda w: len(w) &gt; 0)</code></p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 10:44:40 PM","dateFinished":"Mar 22, 2016 10:44:41 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:97"},{"title":"Get Total Number of Words","text":"%pyspark\n\nprint wordsFiltered.count()     # using a count() action","dateUpdated":"Mar 22, 2016 8:15:50 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190002_1762544573","id":"20160322-194950_370985703","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:15:50 PM","dateFinished":"Mar 22, 2016 8:15:50 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:98"},{"text":"%md\n#### Word Counts\n\nLet's see what are the most popular words by performing a word count.\nUse `map()` and `reduceByKey()` transformations to create tuples of type (word, count). \n#\nReplace `<FILL IN>` with appropriate code.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190002_1762544573","id":"20160322-194950_1011683080","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Word Counts</h4>\n<p>Let's see what are the most popular words by performing a word count.\n<br  />Use <code>map()</code> and <code>reduceByKey()</code> transformations to create tuples of type (word, count).</p>\n<h1></h1>\n<p>Replace <code>&lt;FILL IN&gt;</code> with appropriate code.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:99"},{"title":"Word count with a RDD","text":"%pyspark\n\nwordCounts = wordsFiltered.map(<FILL IN>).reduceByKey(<FILL IN>)","dateUpdated":"Mar 23, 2016 7:58:34 PM","config":{"enabled":true,"title":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190003_1762159824","id":"20160322-194950_1729492261","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:20:54 PM","dateFinished":"Mar 22, 2016 8:20:54 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:100"},{"title":"Solution (click 'Show output')","text":"%md\n`wordCounts = wordsFiltered.map(lambda word: (word, 1)).reduceByKey(lambda a,b: a+b)`","dateUpdated":"Mar 23, 2016 7:58:41 PM","config":{"enabled":true,"title":true,"tableHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190003_1762159824","id":"20160322-194950_1385268482","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>wordCounts = wordsFiltered.map(lambda word: (word, 1)).reduceByKey(lambda a,b: a+b)</code></p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 10:45:06 PM","dateFinished":"Mar 22, 2016 10:45:06 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:101"},{"text":"%md\n#### View Word Count Tuples\nNow let's take a look at top 100 words in descending order with a `takeOrdered()` action.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190003_1762159824","id":"20160322-194950_1337159847","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>View Word Count Tuples</h4>\n<p>Now let's take a look at top 100 words in descending order with a <code>takeOrdered()</code> action.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:102"},{"text":"%pyspark\nprint wordCounts.takeOrdered(100, lambda (w,c): -c)\n","dateUpdated":"Mar 22, 2016 8:20:58 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190003_1762159824","id":"20160322-194950_229182674","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:20:59 PM","dateFinished":"Mar 22, 2016 8:20:59 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103"},{"text":"%md\n#### Filter out infrequent words\nUse `filter()` transformation to filter out words that occur less than five times.\n#\nReplace `<FILL IN>` with appropriate code.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190004_1760236079","id":"20160322-194950_1435022847","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Filter out infrequent words</h4>\n<p>Use <code>filter()</code> transformation to filter out words that occur less than five times.</p>\n<h1></h1>\n<p>Replace <code>&lt;FILL IN&gt;</code> with appropriate code.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:104"},{"text":"%pyspark\n\nfilteredWordCounts = wordCounts.filter(<FILL IN>)","dateUpdated":"Mar 22, 2016 8:49:37 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190004_1760236079","id":"20160322-194950_40150072","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:21:11 PM","dateFinished":"Mar 22, 2016 8:21:11 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:105"},{"title":"Solution (click 'Show output')","text":"%md\n`filteredWordCounts = wordCounts.filter(lambda (w,c): c > 5)`","dateUpdated":"Mar 23, 2016 7:59:10 PM","config":{"enabled":true,"title":true,"tableHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190004_1760236079","id":"20160322-194950_492394946","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>filteredWordCounts = wordCounts.filter(lambda (w,c): c &gt; 5)</code></p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 10:45:28 PM","dateFinished":"Mar 22, 2016 10:45:28 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:106"},{"title":"Take a Look at Results","text":"%pyspark\n\nprint filteredWordCounts.collect()   # we're using a collect() action to pull everything back to the Spark driver","dateUpdated":"Mar 22, 2016 8:18:26 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190004_1760236079","id":"20160322-194950_1427365531","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:18:26 PM","dateFinished":"Mar 22, 2016 8:18:27 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:107"},{"title":"","text":"%md\nNow let's use `countByKey()` action for another way of returning a word count.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"title":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190004_1760236079","id":"20160322-194950_192658880","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now let's use <code>countByKey()</code> action for another way of returning a word count.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:108"},{"text":"%pyspark\n\nresult =  words.map(lambda w: (w,1)).countByKey()\n\n# Print type of data structure\nprint type(result)","dateUpdated":"Mar 22, 2016 8:18:59 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190005_1759851330","id":"20160322-194950_1759940131","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:18:59 PM","dateFinished":"Mar 22, 2016 8:19:00 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"text":"%md\nNote that the **result** is an unordered dictionary of type {word, count}.\nSince this is a small set we can apply a simple (non-parallelizeable) python built-in function.\n","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190005_1759851330","id":"20160322-194950_2021511586","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Note that the <strong>result</strong> is an unordered dictionary of type {word, count}.\n<br  />Since this is a small set we can apply a simple (non-parallelizeable) python built-in function.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:110"},{"text":"%md\nTake a look at first 20 items in our dictionary.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190005_1759851330","id":"20160322-194950_206446715","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Take a look at first 20 items in our dictionary.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:111"},{"text":"%pyspark\n# Print first 20 items\nprint result.items()[0:20]","dateUpdated":"Mar 22, 2016 8:23:25 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190005_1759851330","id":"20160322-194950_1563849100","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:23:25 PM","dateFinished":"Mar 22, 2016 8:23:25 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:112"},{"text":"%md\nApply a python `sorted()` function on the **result** dictionary values.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190005_1759851330","id":"20160322-194950_559468067","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Apply a python <code>sorted()</code> function on the <strong>result</strong> dictionary values.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:113"},{"text":"%pyspark\nimport operator\n\n# Sort in descending order\nsortedResult = sorted(result.items(), key=operator.itemgetter(1), reverse=True)\n\n# Print top 20 items\nprint sortedResult[0:20]","dateUpdated":"Mar 22, 2016 8:23:33 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190006_1761005577","id":"20160322-194950_1296395503","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:23:33 PM","dateFinished":"Mar 22, 2016 8:23:33 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:114"},{"text":"%md\n## Part 2\n#### Introduction to DataFrames and SparkSQL","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190006_1761005577","id":"20160322-194950_150136459","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Part 2</h2>\n<h4>Introduction to DataFrames and SparkSQL</h4>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:115"},{"text":"%md\n#\nIn this section we will cover the concept of a DataFrame. You will convert RDDs from a previous section and then use higher level operations to demonstrate a different way of counting words. Then you will register a temporary table and perform a word count by executing a SQL query on that table.\n#\nBy the end of the section you will have learned higher-level Spark abstractions that hide lower-level details, speed up prototyping and execution. ","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190006_1761005577","id":"20160322-194950_50181188","result":{"code":"SUCCESS","type":"HTML","msg":"<h1></h1>\n<p>In this section we will cover the concept of a DataFrame. You will convert RDDs from a previous section and then use higher level operations to demonstrate a different way of counting words. Then you will register a temporary table and perform a word count by executing a SQL query on that table.</p>\n<h1></h1>\n<p>By the end of the section you will have learned higher-level Spark abstractions that hide lower-level details, speed up prototyping and execution.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116"},{"title":"DataFrame","text":"%md\nA DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. [See SparkSQL docs for more info](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes).","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190006_1761005577","id":"20160322-194950_1581637092","result":{"code":"SUCCESS","type":"HTML","msg":"<p>A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\">See SparkSQL docs for more info</a>.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:117"},{"text":"%md\nTransform your RDD into a DataFrame and perform DataFrame specific operations.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190006_1761005577","id":"20160322-194950_1022022678","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Transform your RDD into a DataFrame and perform DataFrame specific operations.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:118"},{"title":"Word Count with a DataFrame","text":"%pyspark\n\n# First, let's transform our RDD to a DataFrame.\n# We will use a Row to define column names.\nwordsCountsDF = (filteredWordCounts.map(lambda (w, c): \n                Row(word=w,\n                    count=c))\n                .toDF())\n\n# Print schema\nwordsCountsDF.printSchema()\n\n# Output: As you can see, the count and word types have been inferred without having to explicitly define long and string types respectively.","dateUpdated":"Mar 22, 2016 8:25:09 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190007_1760620828","id":"20160322-194950_1560940440","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:25:09 PM","dateFinished":"Mar 22, 2016 8:25:09 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:119"},{"title":"Show top 20 rows","text":"%pyspark\n\n# Show top 20 rows\nwordsCountsDF.show()","dateUpdated":"Mar 22, 2016 8:25:16 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190007_1760620828","id":"20160322-194950_1065743328","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:25:16 PM","dateFinished":"Mar 22, 2016 8:25:17 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:120"},{"title":"Register a Temp Table","text":"%pyspark\n\nwordsCountsDF.registerTempTable(\"word_counts\")","dateUpdated":"Mar 22, 2016 8:25:26 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190007_1760620828","id":"20160322-194950_1934497716","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:25:26 PM","dateFinished":"Mar 22, 2016 8:25:26 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:121"},{"text":"%md\nNow we can query the temporary `word_counts` table with a SQL statement.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190008_1758697084","id":"20160322-194950_200697232","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now we can query the temporary <code>word_counts</code> table with a SQL statement.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:122"},{"text":"%md\nTo execute a SparkSQL query we prepend a block of SQL code with a `%sql` line.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190008_1758697084","id":"20160322-194950_1569052434","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To execute a SparkSQL query we prepend a block of SQL code with a <code>%sql</code> line.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:123"},{"title":"","text":"%sql\n\n-- Display word counts in descending order\nSELECT word, count FROM word_counts ORDER BY count DESC","dateUpdated":"Mar 22, 2016 8:25:32 PM","config":{"enabled":true,"title":false,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"}}},"editorMode":"ace/mode/sql","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190008_1758697084","id":"20160322-194950_2112556725","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:25:32 PM","dateFinished":"Mar 22, 2016 8:25:34 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:124"},{"text":"%md\nNow let's take a step back and perform a word count with SQL","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190008_1758697084","id":"20160322-194950_1393741224","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now let's take a step back and perform a word count with SQL</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:125"},{"title":"Convert RDD to a DataFrame and Register a New Temp Table","text":"%pyspark\n\n# Convert wordsFiltered RDD to a Data Frame\nwordsDF = wordsFiltered.map(lambda w: Row(word=w, count=1)).toDF()","dateUpdated":"Mar 22, 2016 8:25:45 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190008_1758697084","id":"20160322-194950_2004613939","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:25:45 PM","dateFinished":"Mar 22, 2016 8:25:46 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:126"},{"title":"Use DataFrame Specific Functions to Determine Word Counts","text":"%pyspark\n\n(wordsDF.groupBy(\"word\")\n        .sum()\n        .orderBy(\"sum(count)\", ascending=0)\n        .limit(10).show())","dateUpdated":"Mar 22, 2016 8:26:02 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190009_1758312335","id":"20160322-194950_1823018637","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:26:02 PM","dateFinished":"Mar 22, 2016 8:26:07 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:127"},{"title":"Register as Temp Table","text":"%pyspark\n\n# Register as Temp Table\nwordsDF.registerTempTable(\"words\")","dateUpdated":"Mar 22, 2016 8:26:17 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190009_1758312335","id":"20160322-194950_1325040310","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:26:17 PM","dateFinished":"Mar 22, 2016 8:26:17 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:128"},{"text":"%md\n\nNow perform a word count using a SQL statement against the `words` table and order the results in a descending order by count.\n#\nReplace `<FILL IN>` with appropriate code","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190009_1758312335","id":"20160322-194950_547070780","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now perform a word count using a SQL statement against the <code>words</code> table and order the results in a descending order by count.</p>\n<h1></h1>\n<p>Replace <code>&lt;FILL IN&gt;</code> with appropriate code</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:129"},{"text":"%sql\n\nSELECT <FILL IN>","dateUpdated":"Mar 22, 2016 10:46:28 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sql","colWidth":12,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190009_1758312335","id":"20160322-194950_1241106653","dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:26:22 PM","dateFinished":"Mar 22, 2016 8:26:25 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:130"},{"title":"Solution (click 'Show output')","text":"%md\n`SELECT word, count(*) as count FROM words GROUP BY word ORDER BY count DESC`","dateUpdated":"Mar 23, 2016 7:57:15 PM","config":{"enabled":true,"title":true,"tableHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190010_1759466581","id":"20160322-194950_770853660","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>SELECT word, count(*) as count FROM words GROUP BY word ORDER BY count DESC</code></p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:131"},{"text":"%md\n## Part 3\n#### Spark Streaming","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190011_1759081833","id":"20160322-194950_2116104916","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Part 3</h2>\n<h4>Spark Streaming</h4>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:132"},{"text":"%md\n#\nIn this section you will learn how to run a word count on a data in motion rather than static data (or data at rest). The code for a Spark Streaming applications closely resembles that of Part 1. The primary difference is that you will work on micro-batches that will be updated with new data on a pre-defined time interval.\n#\nBy the end of this section you will have downloaded a Spark Streaming application, ran it in a *shell in a browser* and interactively tested the Spark Streaming app by manually typing words in a second shell window running Netcat and observing micro-batch processing on that data as it arrives over internal network connection.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190011_1759081833","id":"20160322-194950_898645245","result":{"code":"SUCCESS","type":"HTML","msg":"<h1></h1>\n<p>In this section you will learn how to run a word count on a data in motion rather than static data (or data at rest). The code for a Spark Streaming applications closely resembles that of Part 1. The primary difference is that you will work on micro-batches that will be updated with new data on a pre-defined time interval.</p>\n<h1></h1>\n<p>By the end of this section you will have downloaded a Spark Streaming application, ran it in a <em>shell in a browser</em> and interactively tested the Spark Streaming app by manually typing words in a second shell window running Netcat and observing micro-batch processing on that data as it arrives over internal network connection.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133"},{"text":"%md\n\n#### Overview\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards.\n\n![Spark Streaming](http://spark.apache.org/docs/1.6.0/img/streaming-arch.png)\n\nInternally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.\n\n![Spark Streaming](http://spark.apache.org/docs/1.6.0/img/streaming-flow.png)\n\n#### DStream\n\nDiscretized Stream or DStream is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark’s abstraction of an immutable, distributed dataset (see [Spark Programming Guide](http://spark.apache.org/docs/1.6.0/programming-guide.html#resilient-distributed-datasets-rdds) for more details). Each RDD in a DStream contains data from a certain interval, as shown in the following figure.\n\n![Spark Streaming](http://spark.apache.org/docs/1.6.0/img/streaming-dstream.png)","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"title":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190011_1759081833","id":"20160322-194950_278143364","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Overview</h4>\n<p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards.</p>\n<p><img src=\"http://spark.apache.org/docs/1.6.0/img/streaming-arch.png\" alt=\"Spark Streaming\" /></p>\n<p>Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.</p>\n<p><img src=\"http://spark.apache.org/docs/1.6.0/img/streaming-flow.png\" alt=\"Spark Streaming\" /></p>\n<h4>DStream</h4>\n<p>Discretized Stream or DStream is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark’s abstraction of an immutable, distributed dataset (see <a href=\"http://spark.apache.org/docs/1.6.0/programming-guide.html#resilient-distributed-datasets-rdds\">Spark Programming Guide</a> for more details). Each RDD in a DStream contains data from a certain interval, as shown in the following figure.</p>\n<p><img src=\"http://spark.apache.org/docs/1.6.0/img/streaming-dstream.png\" alt=\"Spark Streaming\" /></p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:134"},{"text":"%md\n####Now let's proceed to a lab example.","dateUpdated":"Mar 22, 2016 10:40:47 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190011_1759081833","id":"20160322-194950_1831102876","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Now let's proceed to a lab example.</h4>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:135"},{"title":"Run the Following to Download a Spark Streaming Demo to Sandbox","text":"%sh\n\ncd /tmp\n\nif [ -e  spark-streaming-demo.py ]\nthen\n    rm -f spark-streaming-demo.py\nfi\n\nwget https://raw.githubusercontent.com/roberthryniewicz/sample-code/master/spark-streaming-demo.py\n","dateUpdated":"Mar 22, 2016 10:40:50 PM","config":{"enabled":true,"title":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sh","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190011_1757158088","id":"20160322-194950_1905375422","result":{"code":"SUCCESS","type":"TEXT","msg":"--2016-03-22 22:40:50--  https://raw.githubusercontent.com/roberthryniewicz/sample-code/master/spark-streaming-demo.py\nResolving raw.githubusercontent.com... 199.27.79.133\nConnecting to raw.githubusercontent.com|199.27.79.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 683 [text/plain]\nSaving to: “spark-streaming-demo.py”\n\n     0K                                                       100%  143M=0s\n\n2016-03-22 22:40:50 (143 MB/s) - “spark-streaming-demo.py” saved [683/683]\n\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 10:40:50 PM","dateFinished":"Mar 22, 2016 10:40:50 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:136"},{"text":"%md\n#\nHere's how the `spark-streaming-demo.py` code you've just downloaded looks like. Make sure you understand what's going on before you proceed. \n#\nSeveral things worth pointing out:\n1. We've set a 2 sec batch interval to make it easier to inspect results of each batch processed.\n2. We perform a simple word count for each batch and return the results back to the terminal screen with a `pprint()` function.\n#\n\n~~~ python\nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\n\n# Create a local StreamingContext with two working threads and a batch interval of 2 seconds\nsc = SparkContext(\"local[2]\", \"NetworkWordCount\")\nssc = StreamingContext(sc, 2)\n\n# Create a DStream\nlines = ssc.socketTextStream(\"localhost\", 3333)\n\n# Split each line into words\nwords = lines.flatMap(lambda line: line.split(\" \"))\n\n# Count each word in each batch\npairs = words.map(lambda word: (word, 1))\nwordCounts = pairs.reduceByKey(lambda x, y: x + y)\n\n# Print each batch\nwordCounts.pprint()\n\nssc.start()             # Start the computation\nssc.awaitTermination()  # Wait for the computation to terminate\n~~~","dateUpdated":"Mar 22, 2016 8:28:14 PM","config":{"enabled":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190012_1757158088","id":"20160322-194950_368573490","result":{"code":"SUCCESS","type":"HTML","msg":"<h1></h1>\n<p>Here's how the <code>spark-streaming-demo.py</code> code you've just downloaded looks like. Make sure you understand what's going on before you proceed.</p>\n<h1></h1>\n<p>Several things worth pointing out:</p>\n<ol>\n<li>We've set a 2 sec batch interval to make it easier to inspect results of each batch processed.</li>\n<li>We perform a simple word count for each batch and return the results back to the terminal screen with a <code>pprint()</code> function.<h1></h1>\n</li>\n</ol>\n<pre><code class=\"python\">from pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\n\n# Create a local StreamingContext with two working threads and a batch interval of 2 seconds\nsc = SparkContext(\"local[2]\", \"NetworkWordCount\")\nssc = StreamingContext(sc, 2)\n\n# Create a DStream\nlines = ssc.socketTextStream(\"localhost\", 3333)\n\n# Split each line into words\nwords = lines.flatMap(lambda line: line.split(\" \"))\n\n# Count each word in each batch\npairs = words.map(lambda word: (word, 1))\nwordCounts = pairs.reduceByKey(lambda x, y: x + y)\n\n# Print each batch\nwordCounts.pprint()\n\nssc.start()             # Start the computation\nssc.awaitTermination()  # Wait for the computation to terminate\n</code></pre>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:28:13 PM","dateFinished":"Mar 22, 2016 8:28:13 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:137"},{"text":"%md\n#\nNote that any operation applied on a DStream translates to operations on the underlying RDDs. For example, the earlier example above of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. This is shown in the following figure.\n\n![Spark Streaming](http://spark.apache.org/docs/1.6.0/img/streaming-dstream-ops.png)\n\nThese underlying RDD transformations are computed by the Spark engine. The DStream operations hide most of these details and provide the developer with a higher-level API for convenience.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190012_1757158088","id":"20160322-194950_1271808482","result":{"code":"SUCCESS","type":"HTML","msg":"<h1></h1>\n<p>Note that any operation applied on a DStream translates to operations on the underlying RDDs. For example, the earlier example above of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. This is shown in the following figure.</p>\n<p><img src=\"http://spark.apache.org/docs/1.6.0/img/streaming-dstream-ops.png\" alt=\"Spark Streaming\" /></p>\n<p>These underlying RDD transformations are computed by the Spark engine. The DStream operations hide most of these details and provide the developer with a higher-level API for convenience.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:138"},{"text":"%md\n\n#### Submitting a Spark Streaming Job\n\nThis section should be completed within *shell in a browser* since we'll be using a `spark-submit` command to submit our job. \n","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190012_1757158088","id":"20160322-194950_1380418383","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Submitting a Spark Streaming Job</h4>\n<p>This section should be completed within <em>shell in a browser</em> since we'll be using a <code>spark-submit</code> command to submit our job.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:139"},{"text":"%md\n#### First, open a \"Shell in a Box\"\n\nGo to `http://<Host IP>:4200`, for example [http://127.0.0.1:4200](http://127.0.0.1:4200) if you're running a VirtualBox.\n#\n User: `root`\n Pass: `hadoop`\n#\nNext you will be prompted to change your password.\n#\nNote: Your `Host IP` should be `127.0.0.1` for **VirtualBox** and `172.16.148.128` for **VmWare**, unless you're running your Sandbox in the cloud.\n#","dateUpdated":"Mar 23, 2016 7:41:13 PM","config":{"enabled":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190012_1757158088","id":"20160322-194950_1425924540","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>First, open a &ldquo;Shell in a Box&rdquo;</h4>\n<p>Go to <code>http://&lt;Host IP&gt;:4200</code>, for example <a href=\"http://127.0.0.1:4200\">http://127.0.0.1:4200</a> if you're running a VirtualBox.</p>\n<h1></h1>\n<p>User: <code>root</code>\n<br  />Pass: <code>hadoop</code></p>\n<h1></h1>\n<p>Next you will be prompted to change your password.</p>\n<h1></h1>\n<p>Note: Your <code>Host IP</code> should be <code>127.0.0.1</code> for <strong>VirtualBox</strong> and <code>172.16.148.128</code> for <strong>VmWare</strong>, unless you're running your Sandbox in the cloud.</p>\n<h1></h1>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 23, 2016 7:41:13 PM","dateFinished":"Mar 23, 2016 7:41:14 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:140"},{"text":"%md\n#### Second, Submit a Spark Streaming Job\n\nNow you're ready to submit a Spark job. In your terminal window copy and paste the following and hit `Enter`:\n~~~\n/usr/hdp/current/spark-client/bin/spark-submit /tmp/spark-streaming-demo.py\n~~~\n#\nYou should see lots of INFO interspersed with Timestamp corresponding to each batch that is updated every 2 seconds:\n\n~~~\n===========================================\nTime: 2016-03-16 01:26:22\n===========================================\n~~~","dateUpdated":"Mar 22, 2016 8:29:54 PM","config":{"enabled":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190013_1756773339","id":"20160322-194950_1743726907","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Second, Submit a Spark Streaming Job</h4>\n<p>Now you're ready to submit a Spark job. In your terminal window copy and paste the following and hit <code>Enter</code>:</p>\n<pre><code>/usr/hdp/current/spark-client/bin/spark-submit /tmp/spark-streaming-demo.py\n</code></pre>\n<h1></h1>\n<p>You should see lots of INFO interspersed with Timestamp corresponding to each batch that is updated every 2 seconds:</p>\n<pre><code>===========================================\nTime: 2016-03-16 01:26:22\n===========================================\n</code></pre>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:29:52 PM","dateFinished":"Mar 22, 2016 8:29:52 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:141"},{"text":"%md\n#### Next, Run Netcat\n\nNetcat (often abbreviated to nc) is a computer networking utility for reading from and writing to network connections using TCP or UDP.\n#\nIn your browser, open a second tab or window, and open another \"Shell in a Box\" by going to `http://<Host IP>:4200`, for example [http://127.0.0.1:4200](http://127.0.0.1:4200) if you're running a VirtualBox.\n#\nLogin to your shell and run the following command to launch Netcat: `nc -l localhost 3333`\n#\nAt this point you should be connected and you may start typing or pasting any text.\n#\nFor example, if you type the following `hello hello world` text in the Netcat window, you should see the following output in the already running Spark Streaming job tab or window:\n~~~\n===========================================\nTime: 2016-03-16 01:26:24\n===========================================\n(hello, 2)\n(world, 1)\n~~~","dateUpdated":"Mar 23, 2016 7:54:44 PM","config":{"enabled":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190013_1756773339","id":"20160322-194950_108795340","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Next, Run Netcat</h4>\n<p>Netcat (often abbreviated to nc) is a computer networking utility for reading from and writing to network connections using TCP or UDP.</p>\n<h1></h1>\n<p>In your browser, open a second tab or window, and open another &ldquo;Shell in a Box&rdquo; by going to <code>http://&lt;Host IP&gt;:4200</code>, for example <a href=\"http://127.0.0.1:4200\">http://127.0.0.1:4200</a> if you're running a VirtualBox.</p>\n<h1></h1>\n<p>Login to your shell and run the following command to launch Netcat: <code>nc -l localhost 3333</code></p>\n<h1></h1>\n<p>At this point you should be connected and you may start typing or pasting any text.</p>\n<h1></h1>\n<p>For example, if you type the following <code>hello hello world</code> text in the Netcat window, you should see the following output in the already running Spark Streaming job tab or window:</p>\n<pre><code>===========================================\nTime: 2016-03-16 01:26:24\n===========================================\n(hello, 2)\n(world, 1)\n</code></pre>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 23, 2016 7:54:43 PM","dateFinished":"Mar 23, 2016 7:54:43 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:142","focus":true},{"text":"%md\nIf you're done playing around, press `Ctrl + C` to stop your process in either tab/window.","dateUpdated":"Mar 22, 2016 8:31:17 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190013_1756773339","id":"20160322-194950_1014695783","result":{"code":"SUCCESS","type":"HTML","msg":"<p>If you're done playing around, press <code>Ctrl + C</code> to stop your process in either tab/window.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:31:15 PM","dateFinished":"Mar 22, 2016 8:31:15 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:143"},{"text":"%md\n## Optional\nThis section has optional items relating to Spark.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190013_1756773339","id":"20160322-194950_824384054","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Optional</h2>\n<p>This section has optional items relating to Spark.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:144"},{"text":"%md\n#### Explore Spark Jobs via Spark UI\nIf you would like to see the Spark Jobs via the Spark UI go to `http://<Host IP>:4040`, for example [http://127.0.0.1:4040](http://127.0.0.1:4040) on VirtualBox.\n#\nClick on one the jobs, e.g. your word count or streaming job, and checkout the following sections:\n- Timeline view of Spark events\n- Execution DAG\n- Visualization of Spark Streaming statistics\n\nThese will help you better understand what is going on behind the scenes.","dateUpdated":"Mar 22, 2016 8:31:41 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190013_1756773339","id":"20160322-194950_2005788781","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Explore Spark Jobs via Spark UI</h4>\n<p>If you would like to see the Spark Jobs via the Spark UI go to <code>http://&lt;Host IP&gt;:4040</code>, for example <a href=\"http://127.0.0.1:4040\">http://127.0.0.1:4040</a> on VirtualBox.</p>\n<h1></h1>\n<p>Click on one the jobs, e.g. your word count or streaming job, and checkout the following sections:</p>\n<ul>\n<li>Timeline view of Spark events</li>\n<li>Execution DAG</li>\n<li>Visualization of Spark Streaming statistics</li>\n</ul>\n<p>These will help you better understand what is going on behind the scenes.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 22, 2016 8:31:40 PM","dateFinished":"Mar 22, 2016 8:31:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:145"},{"text":"%md\n#### Getting rid of annoying messages\n\nIf you want to remove annoying INFO messages from the Spark streaming terminal window, do the following:\n\nOpen `conf/log4j.properties`, for example: `vi /usr/hdp/current/spark-client/conf/log4j.properties`\n\nand Edit log4j.properties:\n\n~~~\n# Set everything to be logged to the console\nlog4j.rootCategory=INFO, console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n~~~\n\nReplace the first line:\n`log4j.rootCategory=INFO, console`\n  with\n`log4j.rootCategory=WARN, console`\n#\nSave log4j.properties and restart your spark-submit job. Now you should see only **WARN** messages.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190014_1757927586","id":"20160322-194950_93751661","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Getting rid of annoying messages</h4>\n<p>If you want to remove annoying INFO messages from the Spark streaming terminal window, do the following:</p>\n<p>Open <code>conf/log4j.properties</code>, for example: <code>vi /usr/hdp/current/spark-client/conf/log4j.properties</code></p>\n<p>and Edit log4j.properties:</p>\n<pre><code># Set everything to be logged to the console\nlog4j.rootCategory=INFO, console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n</code></pre>\n<p>Replace the first line:\n<br  /><code>log4j.rootCategory=INFO, console</code>\n<br  />with\n<br  /><code>log4j.rootCategory=WARN, console</code></p>\n<h1></h1>\n<p>Save log4j.properties and restart your spark-submit job. Now you should see only <strong>WARN</strong> messages.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:146"},{"title":"The End","text":"%md\nYou've reached the end of this lab!\nWe hope you've been able to successfully complete all the sections and learned a thing or two about Spark.\n\nMake sure to checkout the **Additional Resources** section.","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190014_1757927586","id":"20160322-194950_1578653176","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You've reached the end of this lab!\n<br  />We hope you've been able to successfully complete all the sections and learned a thing or two about Spark.</p>\n<p>Make sure to checkout the <strong>Additional Resources</strong> section.</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147"},{"text":"%md\n### Additional Resources\nThis is just the beggining of your journey with Spark. Make sure to checkout these additional useful resources:\n\n1. [Hortonworks Community Connection](https://hortonworks.com/community/) or HCC -- you'll find guidance, code, examples and best practices to jump start your projects.\n2. [A Lap Around Apache Spark](http://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/) Tutorial\n3. [Apache Spark Programming Guide](http://spark.apache.org/docs/latest/programming-guide.html)\n4. [pySpark Reference Guide](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html)","dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190014_1757927586","id":"20160322-194950_263470347","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Additional Resources</h3>\n<p>This is just the beggining of your journey with Spark. Make sure to checkout these additional useful resources:</p>\n<ol>\n<li><a href=\"https://hortonworks.com/community/\">Hortonworks Community Connection</a> or HCC &ndash; you'll find guidance, code, examples and best practices to jump start your projects.</li>\n<li><a href=\"http://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/\">A Lap Around Apache Spark</a> Tutorial</li>\n<li><a href=\"http://spark.apache.org/docs/latest/programming-guide.html\">Apache Spark Programming Guide</a></li>\n<li><a href=\"https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html\">pySpark Reference Guide</a></li>\n</ol>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148"},{"title":"Feedback","text":"%md\nPlease let us know what you thought of this lab. Your feedback will be incorporated to improve this lab for our future meetups.\n#\n[Give Feedback](http://hor.tn/mylabfeedback)\n#\nThank you!","dateUpdated":"Mar 23, 2016 7:50:37 PM","config":{"enabled":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190014_1757927586","id":"20160322-194950_34841915","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Please let us know what you thought of this lab. Your feedback will be incorporated to improve this lab for our future meetups.</p>\n<h1></h1>\n<p><a href=\"http://hor.tn/mylabfeedback\">Give Feedback</a></p>\n<h1></h1>\n<p>Thank you!</p>\n"},"dateCreated":"Mar 22, 2016 7:49:50 PM","dateStarted":"Mar 23, 2016 7:50:37 PM","dateFinished":"Mar 23, 2016 7:50:37 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:149"},{"dateUpdated":"Mar 22, 2016 7:49:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1458676190014_1757927586","id":"20160322-194950_1472513421","dateCreated":"Mar 22, 2016 7:49:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:150"}],"name":"Lab 101: Intro to Spark with Python","id":"2BE3KD4G6","angularObjects":{"2BF4FYFEE":[],"2BEK3QEPR":[],"2BEUGKP8A":[],"2BED2XA8B":[],"2BFC382WH":[],"2BEDYW971":[],"2BGY6N3UD":[],"2BENYVD9X":[],"2BGFTGT6B":[],"2BFYENWB4":[],"2BFDPRBCM":[],"2BDKJZJ55":[],"2BFUWR97N":[]},"config":{"looknfeel":"default"},"info":{}}