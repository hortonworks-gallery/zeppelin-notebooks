{"paragraphs":[{"text":"%md\n\n## Intro to Spark SQL and DataFrames with Scala\n#### Exploring an Airline Dataset\n@RobHryniewicz\nver 0.5\n","dateUpdated":"Apr 15, 2016 1:23:20 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298133_-1858850818","id":"20160410-003138_1880368561","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Intro to Spark SQL and DataFrames with Scala</h2>\n<h4>Exploring an Airline Dataset</h4>\n<p>@RobHryniewicz\n<br  />ver 0.5</p>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","dateStarted":"Apr 15, 2016 1:23:16 PM","dateFinished":"Apr 15, 2016 1:23:16 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1597"},{"text":"%md\n### Introduction\n\nIn this lab you will use Spark SQL via DataFrames API in Part 1 of the lab and SQL in Part 2 of the lab to explore an Airline Dataset. This is a very interesting dataset that is further explored in a more advanced lab by applying Machine Learning methods for predictive analytics.","dateUpdated":"Apr 10, 2016 10:48:17 AM","config":{"enabled":true,"graph":{"mode":"table","height":217,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298133_-1858850818","id":"20160410-003138_985055475","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Introduction</h3>\n<p>In this lab you will use Spark SQL via DataFrames API in Part 1 of the lab and SQL in Part 2 of the lab to explore an Airline Dataset. This is a very interesting dataset that is further explored in a more advanced lab by applying Machine Learning methods for predictive analytics.</p>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","dateStarted":"Apr 10, 2016 10:48:12 AM","dateFinished":"Apr 10, 2016 10:48:12 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1598"},{"text":"%md\n### Concepts\n\nA DataFrame is a distributed collection of data organized into named columns. \n#\nIt is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. \n#\nDataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. \n#\n**[See SparkSQL docs for more info](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes)**","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298133_-1858850818","id":"20160410-003138_875933602","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Concepts</h3>\n<p>A DataFrame is a distributed collection of data organized into named columns.</p>\n<h1></h1>\n<p>It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.</p>\n<h1></h1>\n<p>DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.</p>\n<h1></h1>\n<p><strong><a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\">See SparkSQL docs for more info</a></strong></p>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1599"},{"text":"%md\n\nThroughout this lab we will use basic Scala syntax. If you would like to learn more about Scala, here's an excellent [Scala Basics Tutorial](http://www.dhgarrette.com/nlpclass/scala/basics.html).","dateUpdated":"Apr 10, 2016 2:07:20 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460297036018_-1630181514","id":"20160410-140356_736870357","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Throughout this lab we will use basic Scala syntax. If you would like to learn more about Scala, here's an excellent <a href=\"http://www.dhgarrette.com/nlpclass/scala/basics.html\">Scala Basics Tutorial</a>.</p>\n"},"dateCreated":"Apr 10, 2016 2:03:56 PM","dateStarted":"Apr 10, 2016 2:07:19 PM","dateFinished":"Apr 10, 2016 2:07:19 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1600"},{"text":"%md\n### Lab Setup & Pre-Check\nBefore we proceed, let's set Spark's external package dependencies and then verify the Spark Version (you should be running at minimum Spark 1.6 for this lab).","dateUpdated":"Apr 10, 2016 10:50:35 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298133_-1858850818","id":"20160410-003138_815648629","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Lab Setup &amp; Pre-Check</h3>\n<p>Before we proceed, let's set Spark's external package dependencies and then verify the Spark Version (you should be running at minimum Spark 1.6 for this lab).</p>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","dateStarted":"Apr 10, 2016 10:50:35 AM","dateFinished":"Apr 10, 2016 10:50:35 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1601"},{"text":"%md\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298133_-1858850818","id":"20160410-003138_1218388802","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1602"},{"title":"Set External Spark Package Dependencies","text":"%dep\n\n// IMPORTANT! \n// This step/paragraph must be executed FIRST; if you have already executed other commands/paragraphs, \n//   please click \"Interpreter\" in the menu above and restart the \"spark\" interpreter and then run this paragraph\n//   before any other one.\n\nz.reset()\nz.load(\"com.databricks:spark-csv_2.11:1.4.0\")   // Spark CSV package","dateUpdated":"Apr 21, 2016 4:29:00 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298134_-1857696571","id":"20160410-003138_1204021069","dateCreated":"Apr 10, 2016 12:31:38 AM","dateStarted":"Apr 21, 2016 4:29:01 AM","dateFinished":"Apr 21, 2016 4:29:08 AM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1603"},{"text":"%md\n**Note**: The first time you run `sc.version` in the paragraph below, several services will initialize in the background. \nThis may take **1~2 min** so please **be patient**. Afterwards, each paragraph should run much more quickly since all the services will already be running.","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298134_-1857696571","id":"20160410-003138_243701002","result":{"code":"SUCCESS","type":"HTML","msg":"<p><strong>Note</strong>: The first time you run <code>sc.version</code> in the paragraph below, several services will initialize in the background.\n<br  />This may take <strong>1~2 min</strong> so please <strong>be patient</strong>. Afterwards, each paragraph should run much more quickly since all the services will already be running.</p>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1604"},{"title":"Check Spark Version","text":"sc.version","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298134_-1857696571","id":"20160410-003138_631425785","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1605"},{"text":"%md ###Start of Lab\n","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298134_-1857696571","id":"20160410-003138_1656468668","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Start of Lab</h3>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1606"},{"text":"%md\nIn the next paragraph we are going to download datasets using shell commands. A shell command in a Zeppelin notebook can can be invoked by \nprepending a block of shell commands with a line containing `%sh` characters.","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298134_-1857696571","id":"20160410-003138_290903368","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In the next paragraph we are going to download datasets using shell commands. A shell command in a Zeppelin notebook can can be invoked by\n<br  />prepending a block of shell commands with a line containing <code>%sh</code> characters.</p>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1607"},{"title":"Download Datasets","text":"%sh\n\n# You will now download a subset of 2008 flights (only 100k lines)\n# The full dataset may be found here: http://stat-computing.org/dataexpo/2009/the-data.html\n\nwget https://raw.githubusercontent.com/roberthryniewicz/datasets/master/airline-dataset/flights/flights.csv -O /tmp/flights.csv\necho \"Downloaded!\"","dateUpdated":"Apr 12, 2016 11:15:39 PM","config":{"enabled":true,"title":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sh","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298134_-1857696571","id":"20160410-003138_1540125404","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1608"},{"title":"Move Datasets to HDFS","text":"%sh\n\n# remove existing copies of dataset from HDFS\nhadoop fs -rm -r -f /tmp/airflightsdelays\n\n# create directory on HDFS\nhadoop fs -mkdir /tmp/airflightsdelays\n\n# put data into HDFS\nhadoop fs -put /tmp/flights.csv /tmp/airflightsdelays/","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298134_-1857696571","id":"20160410-003138_1267267737","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1609"},{"title":"Preview Downloaded File","text":"%sh\nhadoop fs -cat /tmp/airflightsdelays/flights.csv | head","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298134_-1857696571","id":"20160410-003138_226044813","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1610"},{"text":"// Spark Context and Spark SQL Context are automatically initialized in Zeppelin so we will skip those steps\n//val sc: Spark Context\n//val sqlContext: SQL Context\n\n// Create a DataFrame from datasets\nval df = sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\")       // Use first line of all files as header\n    .option(\"inferSchema\", \"true\")  // Automatically infer data types\n    .load(\"/tmp/airflightsdelays/\") // Read all flights","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298134_-1857696571","id":"20160410-003138_236600548","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1611"},{"text":"// Print the schema in a tree format\ndf.printSchema","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298135_-1858081320","id":"20160410-003138_1553179639","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1612"},{"title":"Dataset Description","text":"%angular\n\n<table width=\"100%\">\n<tbody><tr>\n  <th></th>\n  <th>Name</th>\n  <th>Description</th>\n</tr>\n<tr>\n <td>1  </td><td> Year              </td><td>1987-2008</td>\n</tr><tr>\n <td>2  </td><td> Month             </td><td>1-12</td>\n</tr><tr>\n <td>3  </td><td> DayofMonth        </td><td>1-31</td>\n</tr><tr>\n <td>4  </td><td> DayOfWeek         </td><td>1 (Monday) - 7 (Sunday)</td>\n</tr><tr>\n <td>5  </td><td> DepTime           </td><td>actual departure time (local, hhmm)</td>\n</tr><tr>\n <td>6  </td><td> CRSDepTime        </td><td>scheduled departure time (local, hhmm)</td>\n</tr><tr>\n <td>7  </td><td> ArrTime           </td><td>actual arrival time (local, hhmm)</td>\n</tr><tr>\n <td>8  </td><td> CRSArrTime        </td><td>scheduled arrival time (local, hhmm)</td>\n</tr><tr>\n <td>9  </td><td> UniqueCarrier     </td><td><a href=\"supplemental-data.html\">unique carrier code</a></td>\n</tr><tr>\n <td>10 </td><td> FlightNum         </td><td>flight number</td>\n</tr><tr>\n <td>11 </td><td> TailNum           </td><td>plane tail number</td>\n</tr><tr>\n <td>12 </td><td> ActualElapsedTime </td><td>in minutes</td>\n</tr><tr>\n <td>13 </td><td> CRSElapsedTime    </td><td>in minutes</td>\n</tr><tr>\n <td>14 </td><td> AirTime           </td><td>in minutes</td>\n</tr><tr>\n <td>15 </td><td> ArrDelay          </td><td>arrival delay, in minutes</td>\n</tr><tr>\n <td>16 </td><td> DepDelay          </td><td>departure delay, in minutes</td>\n</tr><tr>\n <td>17 </td><td> Origin            </td><td>origin <a href=\"supplemental-data.html\">IATA airport code</a></td>\n</tr><tr>\n <td>18 </td><td> Dest              </td><td>destination <a href=\"supplemental-data.html\">IATA airport code</a></td>\n</tr><tr>\n <td>19 </td><td> Distance          </td><td>in miles</td>\n</tr><tr>\n <td>20 </td><td> TaxiIn            </td><td>taxi in time, in minutes</td>\n</tr><tr>\n <td>21 </td><td> TaxiOut           </td><td>taxi out time in minutes</td>\n</tr><tr>\n <td>22 </td><td> Cancelled           </td><td>was the flight cancelled?</td>\n</tr><tr>\n <td>23 </td><td> CancellationCode  </td><td>reason for cancellation (A = carrier, B = weather, C = NAS, D = security)</td>\n</tr><tr>\n <td>24 </td><td> Diverted          </td><td>1 = yes, 0 = no</td>\n</tr><tr>\n <td>25 </td><td> CarrierDelay      </td><td>in minutes</td>\n</tr><tr>\n <td>26 </td><td> WeatherDelay      </td><td>in minutes</td>\n</tr><tr>\n <td>27 </td><td> NASDelay          </td><td>in minutes</td>\n</tr><tr>\n <td>28 </td><td> SecurityDelay     </td><td>in minutes</td>\n</tr><tr>\n <td>29 </td><td> LateAircraftDelay </td><td>in minutes</td>\n</tr>\n</tbody></table>","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298135_-1858081320","id":"20160410-003138_1626463388","result":{"code":"SUCCESS","type":"ANGULAR","msg":"<table width=\"100%\">\n<tbody><tr>\n  <th></th>\n  <th>Name</th>\n  <th>Description</th>\n</tr>\n<tr>\n <td>1  </td><td> Year              </td><td>1987-2008</td>\n</tr><tr>\n <td>2  </td><td> Month             </td><td>1-12</td>\n</tr><tr>\n <td>3  </td><td> DayofMonth        </td><td>1-31</td>\n</tr><tr>\n <td>4  </td><td> DayOfWeek         </td><td>1 (Monday) - 7 (Sunday)</td>\n</tr><tr>\n <td>5  </td><td> DepTime           </td><td>actual departure time (local, hhmm)</td>\n</tr><tr>\n <td>6  </td><td> CRSDepTime        </td><td>scheduled departure time (local, hhmm)</td>\n</tr><tr>\n <td>7  </td><td> ArrTime           </td><td>actual arrival time (local, hhmm)</td>\n</tr><tr>\n <td>8  </td><td> CRSArrTime        </td><td>scheduled arrival time (local, hhmm)</td>\n</tr><tr>\n <td>9  </td><td> UniqueCarrier     </td><td><a href=\"supplemental-data.html\">unique carrier code</a></td>\n</tr><tr>\n <td>10 </td><td> FlightNum         </td><td>flight number</td>\n</tr><tr>\n <td>11 </td><td> TailNum           </td><td>plane tail number</td>\n</tr><tr>\n <td>12 </td><td> ActualElapsedTime </td><td>in minutes</td>\n</tr><tr>\n <td>13 </td><td> CRSElapsedTime    </td><td>in minutes</td>\n</tr><tr>\n <td>14 </td><td> AirTime           </td><td>in minutes</td>\n</tr><tr>\n <td>15 </td><td> ArrDelay          </td><td>arrival delay, in minutes</td>\n</tr><tr>\n <td>16 </td><td> DepDelay          </td><td>departure delay, in minutes</td>\n</tr><tr>\n <td>17 </td><td> Origin            </td><td>origin <a href=\"supplemental-data.html\">IATA airport code</a></td>\n</tr><tr>\n <td>18 </td><td> Dest              </td><td>destination <a href=\"supplemental-data.html\">IATA airport code</a></td>\n</tr><tr>\n <td>19 </td><td> Distance          </td><td>in miles</td>\n</tr><tr>\n <td>20 </td><td> TaxiIn            </td><td>taxi in time, in minutes</td>\n</tr><tr>\n <td>21 </td><td> TaxiOut           </td><td>taxi out time in minutes</td>\n</tr><tr>\n <td>22 </td><td> Cancelled           </td><td>was the flight cancelled?</td>\n</tr><tr>\n <td>23 </td><td> CancellationCode  </td><td>reason for cancellation (A = carrier, B = weather, C = NAS, D = security)</td>\n</tr><tr>\n <td>24 </td><td> Diverted          </td><td>1 = yes, 0 = no</td>\n</tr><tr>\n <td>25 </td><td> CarrierDelay      </td><td>in minutes</td>\n</tr><tr>\n <td>26 </td><td> WeatherDelay      </td><td>in minutes</td>\n</tr><tr>\n <td>27 </td><td> NASDelay          </td><td>in minutes</td>\n</tr><tr>\n <td>28 </td><td> SecurityDelay     </td><td>in minutes</td>\n</tr><tr>\n <td>29 </td><td> LateAircraftDelay </td><td>in minutes</td>\n</tr>\n</tbody></table>"},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1613"},{"text":"%md\n### Part 1: Using DataFrames API to Analyze Airline Dataset","dateUpdated":"Apr 10, 2016 10:52:31 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298135_-1858081320","id":"20160410-003138_650819453","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Part 1: Using DataFrames API to Analyze Airline Dataset</h3>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","dateStarted":"Apr 10, 2016 10:52:16 AM","dateFinished":"Apr 10, 2016 10:52:16 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1614"},{"title":"Part 1: Using DataFrames to Analyze Dataset","text":"// Show a subset of columns with \"select\"\ndf.select(\"UniqueCarrier\", \"FlightNum\", \"DepDelay\", \"ArrDelay\", \"Distance\").show","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298135_-1858081320","id":"20160410-003138_1188332400","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1615"},{"text":"// Create a DataFrame containing Flights with delayed Departure by more than 15 min using \"filter\"\nval delayedDF = df.select(\"UniqueCarrier\", \"DepDelay\").filter($\"DepDelay\" > 15).cache\ndelayedDF.show","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298135_-1858081320","id":"20160410-003138_704729700","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1616"},{"text":"// Print total number of delayed flights\nprintln(\"Total Number of Delayed Flights: \" + delayedDF.count)","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298135_-1858081320","id":"20160410-003138_1019754695","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1617"},{"title":"Define a UDF to Determine Delays","text":"import org.apache.spark.sql.functions.udf\n\n// Define a UDF to find delayed flights\n\n// Assume:\n//  if ArrDelay is not available then Delayed = False\n//  if ArrDelay > 15 min then Delayed = True else False\n\nval isDelayedUDF = udf((time: String) => if (time == \"NA\") 0 else if (time.toInt > 15) 1 else 0)","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298135_-1858081320","id":"20160410-003138_2097655805","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1618"},{"text":"// Define a new DataFrame that contains a subset of the original columns and a new column \"IsDelayed\" by applying a UDF\n// isDelayed()  on \"DepDelay\" column\n\nval updatedDF = df.select($\"Year\", $\"Month\", $\"DayofMonth\", $\"DayOfWeek\", $\"CRSDepTime\", $\"UniqueCarrier\", $\"FlightNum\", \n                    $\"DepDelay\", $\"Origin\", $\"Dest\", $\"TaxiIn\", $\"TaxiOut\", $\"Distance\",\n                    isDelayedUDF($\"DepDelay\").alias(\"IsDelayed\")).cache\n\nupdatedDF.show // Notice new column \"IsDelayed\"","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298135_-1858081320","id":"20160410-003138_62081836","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1619"},{"title":"Calculate Percentage of Delayed Flights","text":"updatedDF.agg((sum(\"IsDelayed\") * 100 / count(\"DepDelay\")).alias(\"Percentage of Delayed Flights\")).show","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298136_-1860005065","id":"20160410-003138_1879848857","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1620"},{"title":"Find Avg Taxi-out","text":"// Show only Origin, Dest, and TaxiOut columns\nupdatedDF.select(\"Origin\", \"Dest\", \"TaxiOut\").groupBy(\"Origin\", \"Dest\").agg(avg(\"TaxiOut\").alias(\"AvgTaxiOut\")).orderBy(desc(\"AvgTaxiOut\")).show(10)","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298136_-1860005065","id":"20160410-003138_840324935","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1621"},{"text":"%md\n\nIn the next paragraph replace `<FILL IN>` with your own code. Reference [SparkSQL documentation](https://spark.apache.org/docs/1.1.0/sql-programming-guide.html) and previous examples.","dateUpdated":"Apr 21, 2016 4:30:13 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248464619_1041079907","id":"20160410-003424_739062063","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In the next paragraph replace <code>&lt;FILL IN&gt;</code> with your own code. Reference <a href=\"https://spark.apache.org/docs/1.1.0/sql-programming-guide.html\">SparkSQL documentation</a> and previous examples.</p>\n"},"dateCreated":"Apr 10, 2016 12:34:24 AM","dateStarted":"Apr 10, 2016 12:38:50 AM","dateFinished":"Apr 10, 2016 12:38:51 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1622"},{"title":"Find Avg Taxi-in","text":"// Find average TaxiIn\n// Show only Origin, Dest, and TaxiIn columns\n// Use aggregate functions on the updatedDF DataFrame to complete the code\n\nupdatedDF.<FILL IN>","dateUpdated":"Apr 21, 2016 4:30:21 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298136_-1860005065","id":"20160410-003138_1488719873","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1623"},{"title":"SOLUTION (click 'Show output', i.e. closed notepad icon on the right)","text":"%md\n\n`updatedDF.select(\"Origin\", \"Dest\", \"TaxiIn\").groupBy(\"Origin\", \"Dest\").agg(avg(\"TaxiIn\").alias(\"AvgTaxiIn\")).orderBy(desc(\"AvgTaxiIn\")).show(10)`","dateUpdated":"Apr 13, 2016 2:52:55 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248529430_823170823","id":"20160410-003529_382090485","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>updatedDF.select(\"Origin\", \"Dest\", \"TaxiIn\").groupBy(\"Origin\", \"Dest\").agg(avg(\"TaxiIn\").alias(\"AvgTaxiIn\")).orderBy(desc(\"AvgTaxiIn\")).show(10)</code></p>\n"},"dateCreated":"Apr 10, 2016 12:35:29 AM","dateStarted":"Apr 10, 2016 12:37:24 AM","dateFinished":"Apr 10, 2016 12:37:25 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1624"},{"text":"%md\n### Part 2: Using SQL to Analyze the Airline Dataset","dateUpdated":"Apr 10, 2016 10:53:25 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298136_-1860005065","id":"20160410-003138_582934314","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Part 2: Using SQL to Analyze the Airline Dataset</h3>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","dateStarted":"Apr 10, 2016 10:53:24 AM","dateFinished":"Apr 10, 2016 10:53:25 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1625"},{"title":"","text":"%md\nNow let's use SQL statements to analyze our dataset.","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298136_-1860005065","id":"20160410-003138_556617784","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now let's use SQL statements to analyze our dataset.</p>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1626"},{"title":"Register a Temporary Table","text":"// Convert DataFrame to a Temporary Table\nupdatedDF.registerTempTable(\"flightsTempTbl\")","dateUpdated":"Apr 10, 2016 12:39:30 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298136_-1860005065","id":"20160410-003138_636329356","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Apr 10, 2016 12:31:38 AM","dateStarted":"Apr 10, 2016 12:39:22 AM","dateFinished":"Apr 10, 2016 12:39:22 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1627"},{"title":"Preview Table","text":"%sql\n\nSELECT * FROM flightsTempTbl LIMIT 10","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sql","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298137_-1860389814","id":"20160410-003138_318924232","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1628"},{"title":"Register UDF","text":"// Register a UDF to find delayed flights\n// Note that this is a UDF specific for use within the sqlContext\n\n// Assume:\n//  if ArrDelay is not available then Delayed = False\n//  if ArrDelay > 15 min then Delayed = True else False\n\nsqlContext.udf.register(\"isDelayedUDF\", (time: String) => if (time == \"NA\") 0 else if (time.toInt > 15) 1 else 0)","dateUpdated":"Apr 10, 2016 12:40:51 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298137_-1860389814","id":"20160410-003138_40384312","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1629"},{"title":"Compare Total Number of Delayed Flights by Carrier","text":"%sql\n\nSELECT UniqueCarrier, SUM(isDelayedUDF(DepDelay)) AS NumDelays FROM flightsTempTbl GROUP BY UniqueCarrier","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"pieChart","height":296,"optionOpen":false,"keys":[{"name":"UniqueCarrier","index":0,"aggr":"sum"}],"values":[{"name":"NumDelays","index":1,"aggr":"sum"}],"groups":[],"scatter":{"yAxis":{"name":"NumDelays","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/sql","colWidth":6},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298137_-1860389814","id":"20160410-003138_134299332","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1630"},{"title":"Compare Total Delayed Time (min) by Carrier","text":"%sql\n\nSELECT UniqueCarrier, SUM(DepDelay) AS TotalTimeDelay FROM flightsTempTbl GROUP BY UniqueCarrier","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"pieChart","height":300,"optionOpen":false,"keys":[{"name":"UniqueCarrier","index":0,"aggr":"sum"}],"values":[{"name":"TotalTimeDelay","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"UniqueCarrier","index":0,"aggr":"sum"},"yAxis":{"name":"TotalTimeDelay","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/sql","colWidth":6},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298137_-1860389814","id":"20160410-003138_163559927","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1631"},{"text":"%md\n\nIn the paragraph below replace `<FILL IN>` with your code.","dateUpdated":"Apr 10, 2016 12:43:01 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248881653_1928860897","id":"20160410-004121_2117120539","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In the paragraph below replace <code>&lt;FILL IN&gt;</code> with your code.</p>\n"},"dateCreated":"Apr 10, 2016 12:41:21 AM","dateStarted":"Apr 10, 2016 12:43:00 AM","dateFinished":"Apr 10, 2016 12:43:00 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1632"},{"title":"Find Average Distance Travelled by Carrier","text":"%sql\n\n-- Find average distance by UniqueCarrier from flightsTempTbl\n-- order descending by average distance\n\nSELECT <FILL IN>","dateUpdated":"Apr 10, 2016 10:54:02 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sql","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298137_-1860389814","id":"20160410-003138_172624929","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1633"},{"title":"SOLUTION (click 'Show output', i.e. closed notepad icon on the right)","text":"%md\n\n`UniqueCarrier, avg(Distance) AS AvgDistanceTraveled FROM flightsTempTbl GROUP BY UniqueCarrier ORDER BY AvgDistanceTraveled DESC`","dateUpdated":"Apr 10, 2016 12:46:23 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460249108342_-53271323","id":"20160410-004508_2129481983","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>UniqueCarrier, avg(Distance) AS AvgDistanceTraveled FROM flightsTempTbl GROUP BY UniqueCarrier ORDER BY AvgDistanceTraveled DESC</code></p>\n"},"dateCreated":"Apr 10, 2016 12:45:08 AM","dateStarted":"Apr 10, 2016 12:45:17 AM","dateFinished":"Apr 10, 2016 12:45:18 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1634"},{"title":"Find Out When Most Flights Get Delayed by Day of Week","text":"%sql\n\nSELECT DayOfWeek, CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END AS Delay, COUNT(1) AS Count\nFROM flightsTempTbl\nGROUP BY DayOfWeek, CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END\nORDER BY DayOfWeek","dateUpdated":"Apr 10, 2016 12:48:03 AM","config":{"enabled":true,"title":true,"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"DayOfWeek","index":0,"aggr":"sum"}],"values":[{"name":"Count","index":2,"aggr":"sum"}],"groups":[{"name":"Delay","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"DayOfWeek","index":0,"aggr":"sum"},"yAxis":{"name":"Delay","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/sql","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298137_-1860389814","id":"20160410-003138_56774606","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1635"},{"title":"Find Out When Most Flights Get Delayed by Hour","text":"%sql\n\nSELECT CAST(CRSDepTime / 100 AS INT) AS Hour, CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END AS Delay, COUNT(1) AS Count\nFROM flightsTempTbl\nGROUP BY CAST(CRSDepTime / 100 AS INT), CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END\nORDER BY Hour","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"Hour","index":0,"aggr":"sum"}],"values":[{"name":"Count","index":2,"aggr":"sum"}],"groups":[{"name":"Delay","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"Hour","index":0,"aggr":"sum"},"yAxis":{"name":"Delay","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/sql","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298137_-1860389814","id":"20160410-003138_728063774","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1636"},{"text":"%angular\n\n<h3>Save Modes</h3>\n\nSave operations can optionally take a <code>SaveMode</code>, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing a <code>Overwrite</code>, the data will be deleted before writing out the new data.\n<br><br>\n<style>\ntable, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n}\nth, td {\n    padding: 5px;\n}\n</style>\n\n<table style=\"width:100%\">\n  <tr>\n    <th>Mode (Scala/Java)</th>\n    <th>Meaning</th>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.ErrorIfExists (default)</code></td>\n    <td>When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td>\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Append</code></td>\n    <td>When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.</td>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Overwrite</code></td>\n    <td>Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Ignore</code></td>\n    <td>Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.</td>\n  </tr>\n</table>","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298138_-1859235567","id":"20160410-003138_206029012","result":{"code":"SUCCESS","type":"ANGULAR","msg":"<h3>Save Modes</h3>\n\nSave operations can optionally take a <code>SaveMode</code>, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing a <code>Overwrite</code>, the data will be deleted before writing out the new data.\n<br><br>\n<style>\ntable, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n}\nth, td {\n    padding: 5px;\n}\n</style>\n\n<table style=\"width:100%\">\n  <tr>\n    <th>Mode (Scala/Java)</th>\n    <th>Meaning</th>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.ErrorIfExists (default)</code></td>\n    <td>When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td>\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Append</code></td>\n    <td>When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.</td>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Overwrite</code></td>\n    <td>Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td>\t\t\n  </tr>\n  <tr>\n    <td><code>SaveMode.Ignore</code></td>\n    <td>Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.</td>\n  </tr>\n</table>"},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1637"},{"title":"Save to ORC file","text":"import org.apache.spark.sql.SaveMode\n\n// Save and Overwrite results to an ORC file\nupdatedDF.write.format(\"orc\").mode(SaveMode.Overwrite).save(\"flightsAndDelays.orc\")","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298138_-1859235567","id":"20160410-003138_985965720","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1638"},{"title":"Load from ORC file","text":"// Load results back from ORC file\nval dfTest = sqlContext.read.format(\"orc\").load(\"flightsAndDelays.orc\")\ndfTest.show","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298138_-1859235567","id":"20160410-003138_1142035788","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1639"},{"title":"Compare DataFrame Sizes (i.e. Compare Original DataFrame with one Loaded from HDFS)","text":"// Note: if output assertion succeeds no warning messages will be printed\nassert (dfTest.count == updatedDF.count, println(\"Assertion Fail: Files are of different sizes.\"))","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298138_-1859235567","id":"20160410-003138_2134135677","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1640"},{"text":"%md\n\n### Saving to Persistent Tables\nUnlike the `registerTempTable` command, `saveAsTable` will materialize the contents of the dataframe and create a pointer to the data in the HiveMetastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the `table` method on a `SQLContext` with the name of the table.<br>\nBy default `saveAsTable` will create a “managed table”, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped.<br>","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298138_-1859235567","id":"20160410-003138_1146451145","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Saving to Persistent Tables</h3>\n<p>Unlike the <code>registerTempTable</code> command, <code>saveAsTable</code> will materialize the contents of the dataframe and create a pointer to the data in the HiveMetastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the <code>table</code> method on a <code>SQLContext</code> with the name of the table.<br>\n<br  />By default <code>saveAsTable</code> will create a “managed table”, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped.<br></p>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1641"},{"title":"Save to a Table","text":"updatedDF.write.format(\"orc\").mode(SaveMode.Overwrite).saveAsTable(\"flightsPermTbl\")","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298138_-1859235567","id":"20160410-003138_1181113131","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1642"},{"title":"Read from Table to a New DataFrame","text":"val dfFromTbl = sqlContext.table(\"flightsPermTbl\")","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298138_-1859235567","id":"20160410-003138_1777043496","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1643"},{"title":"Show Elements of DataFrame","text":"dfFromTbl.show(5)","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298139_-1859620316","id":"20160410-003138_1714958443","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1644"},{"title":"Show Tables","text":"%sql\n\nSHOW Tables\n\n-- Notice that unlike flightsTempTbl, flightsPermTbl is a permanent table","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"tableName","index":0,"aggr":"sum"}],"values":[{"name":"isTemporary","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"tableName","index":0,"aggr":"sum"},"yAxis":{"name":"isTemporary","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/sql","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298139_-1859620316","id":"20160410-003138_1928525114","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1645"},{"title":"Drop Managed Table","text":"%sql\n\nDROP TABLE flightsPermTbl","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298139_-1859620316","id":"20160410-003138_750906827","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1646"},{"title":"Describe Table","text":"%sql\n\nSHOW tables\n\n-- notice that flightsPermTbl is no longer available","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"tableName","index":0,"aggr":"sum"}],"values":[{"name":"isTemporary","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"tableName","index":0,"aggr":"sum"},"yAxis":{"name":"isTemporary","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/sql","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298139_-1859620316","id":"20160410-003138_1877131617","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1647"},{"title":"The End","text":"%md\nYou've reached the end of this lab! We hope you've been able to successfully complete all portions of this lab.","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298139_-1859620316","id":"20160410-003138_268828842","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You've reached the end of this lab! We hope you've been able to successfully complete all portions of this lab.</p>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1648"},{"text":"%md\n### Additional Resources\nThis is just the beggining of your journey with Spark. Make sure to checkout these additional useful resources:\n\n1. [Hortonworks Community Connection](https://hortonworks.com/community/) (HCC) for guidance, code, examples and best practices to jump start your projects.\n2. [Spark SQL, DataFrames and DataSets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n3. [A Lap Around Spark](http://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/)","dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298139_-1859620316","id":"20160410-003138_2048237853","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Additional Resources</h3>\n<p>This is just the beggining of your journey with Spark. Make sure to checkout these additional useful resources:</p>\n<ol>\n<li><a href=\"https://hortonworks.com/community/\">Hortonworks Community Connection</a> (HCC) for guidance, code, examples and best practices to jump start your projects.</li>\n<li><a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\">Spark SQL, DataFrames and DataSets Guide</a></li>\n<li><a href=\"http://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/\">A Lap Around Spark</a></li>\n</ol>\n"},"dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1649"},{"dateUpdated":"Apr 10, 2016 12:31:38 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460248298140_-1861544060","id":"20160410-003138_1663715025","dateCreated":"Apr 10, 2016 12:31:38 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1650"}],"name":"Lab 102: Intro to Spark with Scala","id":"2BJVW65WS","angularObjects":{"2BF4FYFEE":[],"2BEK3QEPR":[],"2BEUGKP8A":[],"2BED2XA8B":[],"2BFC382WH":[],"2BEDYW971":[],"2BGY6N3UD":[],"2BENYVD9X":[],"2BGFTGT6B":[],"2BFYENWB4":[],"2BFDPRBCM":[],"2BDKJZJ55":[],"2BFUWR97N":[]},"config":{"looknfeel":"default"},"info":{}}
